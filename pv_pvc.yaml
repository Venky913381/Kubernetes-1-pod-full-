>
*READ_FIRST:-
>Volumes [EmptyDir/HostPath]

>What problems does it solve?

    Containers running inside the pod can not share the files with each other, if they have volume mounted 
    on different hosts and if they are not in sync with each other.

    All the files inside the container are temporary which means if you terminate the container,
    you are going to lose all your files.(in case of EmptyDir)

    Secondly, if in any case, your container crashes then there is no way to recover files.
    Kuberenetes provides volume plugin as Persistent Volume to address the above problems.

    The lifecycle of these volumes are independent of the lifecycle of pods. (host path)
    So if PODs are terminated then volumes are unmounted and detached keeping the data intact.


>What is Persistent Volume(PV)?

    In simple terms, it's storage/pool of storage volumes, available within your Kubernetes cluster. 
    This storage can be provisioned by you or Kubernetes administrator.

    It's basically a directory with some data in it and all the containers running inside the pods can access it. 
    But Persistent Volumes are independent of the POD life cycle.

    it is a cluster wide resource, used to store /persist data beyond the lifetime of a pod.

    So if PODs live or die, persistent volume does not gets affected and it can be reused by some other PODs.

    >  Kubernetes provides many volume plugins based on the cloud service provider you are using ___

        awsElasticBlockStore, azureDisk, azureFile, cephfs, cinder, configMap, csi, downwardAPI, 
        emptyDir, fc (fibre channel), flexVolume, flocker, gcePersistentDisk, gitRepo (deprecated), 
        glusterfs, hostPath, iscsi, local, nfs, persistentVolumeClaim, projected, portworxVolume, 
        quobyte, rbd, scaleIO, secret, storageos, vsphereVolume


>What is Persistent Volume claim(PVC?

    Persistent volume claim provides you an abstraction between the consumption of the storage 
    and implementation of the storage.
    In the nutshell you can say, its a request for storage on behalf of an application which is running on cluster.
    .i.e. when any pod lay a claim on volume, and if that storage is available then it will get attached to that pod.


>---------------------------------------------------------------------------------------------------------------

>flow :-   POD------>PVC------->PV----->h/w storage

1. create/execute manifest for volume i.e provision the required PV 
2. bind the volume to PVC i.e. execute manifest for pvc 
3. Create the pod and attach the pvc to it. 

*PV_

apiVersion: v1
kind: PersistentVolume
metadata:
  name: jhooq-demo-pv         # name will be referenced by pvc
  labels:                     # label is optional
    type: awsEBS
spec:
  capacity:
    storage: 10Gi
  volumeMode: Filesystem       #filesystem or Block
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain     #other, Delete or Manual, if cluster is not provisned on cloud
  storageClassName: local-storage             
  local:                                    #or-->  hostPath:
    path: /home/vagrant/storage             #          path: "/path"


  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname   # pv will be provisioned on the node with given key as a label attached to it
          operator: In
          values:
          - node1



*PVC_

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jhooq-pvc          # name will be referenced by storage claiming pod
spec:
  volumeName: jhooq-demo-pv           # name of the pv
  storageClassName: local-storage     # must need to same as describe in PV specs, it will decide which PV 
  volumeMode: Filesystem              # will get served the claim.
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi         # 1gib out of 10gib provisioned by pv.

  selector:                # selector is optional, needed only in case of selection of perticulat pv
    matchLabels: 
      type: "awsEBS"    



*POD_

apiVersion: v1
kind: Pod
metadata:
  name: jhooq-pod-with-pvc
  labels:
    name: jhooq-pod-with-pvc
spec:
  containers:
  - name: jhooq-pod-with-pvc
    image: rahulwagh17/kubernetes:jhooq-k8s-springboot
    ports:
      - containerPort: 8080
        name: www

    volumeMounts:
      - name: www-persistent-storage
        mountPath: /home/vagrant/storage

  volumes:                               #Attaching the claim to the pod
    - name: www-persistent-storage
      persistentVolumeClaim:
        claimName: jhooq-pvc             #Name of the pvc



>kubectl get pv
>kubectl get pvc
>kubectl describe pv <pv_name>
>kubectl describe pvc <pvc_name>

>If there are multiple PV's and if we do (not) exclusively mentioned the volumeName in pvc, 
>then k8s will check, what pv will satisfy the storage need as per pvc and automatically those will
>be bind together.


> AccessModes, can be_ (from pv/pvc@50min mark)

     ReadWriteOnce (RWO) : only single worker "NODE" can mount the volume for reading and writing at same time.
                           this mode will still allow multiple PODS to access the volume when pods are running
                           on SAME NODE on which PV is provisioned on.
                           (for pvc, is the only mode supported, if storage class is manual)

     ReadOnlyMany (ROX)  : the volume can be mounted as read-only by MANY NODES at the same time.
                           (for pvc, do not support if storage class is manual, available only when cluster is 
                           provisioned on cloud)

     ReadWriteMany (RWX) : the volume can be mounted as read-write by MANY NODES at the same time. 
                           (for pvc, do not support if storage class is manual, available only when cluster is 
                           provisioned on cloud)

*note_  not all cloud provider support all above modes, go through provider docx for more details.


*ref_https://alexandre-vazquez.com/untangling-reclaim-policies-for-persistent-volume-in-kubernetes/

>persistentVolumeReclaimPolicy, can be_

  >Reclaim policy tell us , what happens to persistent volume , when the PVC is deleted.

     Delete  : it deletes volume content/data and makes the volume available to be claimed again as 
               soon as pvc gets deleted. 
               Delete reclaim policy deletes both the PersistentVolume object from your cluster and 
               the associated storage asset in external infrastructure, such as AWS EBS

     Retain  : pv content/data will be persisted after pvc is deleted and it(data) can NOT be reused, until
               administrator reclaim the volume manually.

     Recycle : this policy will remove everything from the perticular device. this policy is valid
               only for NFS and HostPath type.           
               Recycle reclaim policy recycles the volume back into the pool of unbound persistent volumes 
               once it is released from its claim. (this policy is Deprecated)


changing the reclaim policy using cli :
>kubectl patch pv <your-pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'


>--------------------------------------------------------------------------------------------------------------- 


>Pros and Cons_
  After using the persistent volume and persistent volume claim, 
  I must say its always beneficial to use both when you are working in the production environment because
  you can not just delete the data after the end of the POD cycle.

  The most favorable use case which I can see setting up the database such as
  MySQL, Oracle, Postgress inside the persistent volume so that it is always there irrespective 
  of your POD life cycle.

>Pros_
  Storing and archiving the logs
  Setting up the database
  Useful for application handling a large number of batch jobs
  Storing configs of application
  Independent from PODs life-cycle
  Easy to use
  Easy to backup

>Cons_
  Can not be used to store transnational data for performance-intensive application
  You need to set up your own backup policies
  Couldn't be used for HA(High Availability)

>Conclusion_
  If you are reading this part of the blog post then I must say you have at least 
  implemented PV and PVC inside your Kubernetes cluster. But to conclude it here is recap of what we did -

  Gone through the concepts of "What is Persistent Volume and Persistent Volume Claim"
  Then we created a Persistent Volume .i.e - jhooq-demo-pv with 1 Gi of storage
  Created the Persistent Volume Claim .i.e. - jhooq-pvc to use persistent volume jhooq-demo-pv
  Finally created the POD and deployed spring boot microservice.


>---------------------------------------------------------------------------------------------------------------

>kubernetes solves the problems associated with EmptyDir/ HostPath by using PV and PVC's.

>Persistance volume data is persisted regardless of the lifecycle of Application, Pod, Node or
>even Cluster itself. it represents the storage option that is used to persist application data.

>pv has its own lifecycle, seperate from lifecycle of k8s pod, it essentailly consist of two things_
1. a backend technology called persiatnce Volume (local, ebs, azureDisk, NFS etc...)
2. an access mode, which tells k8s how the volumes should be mounted

>Admin creates a pool of persistant volumes/PV's for user to choose from. pv can have varying properties, 
>such a performance for different problems.
>it is a cluster wide resource used to store /persist data beyond the lifetime of a pod.

>pv's are not generally backed by any locally attached storage on worker node but by networked storage system 
>such as cloud providers storage or NFS or distributed filesystem like Ceph or GlusterFS. 

>PV provide a file system that can be mounted on to the cluster, without being associated with any 
>perticular node.

>it act as an abstraction layer to save the user from going in to the details of storage is managed
>and provisioned by each cloud provider. This api object captures the details of the implemantation
>of the storage , be that NFS, iSCSI or cloud provider specific storage system.

>advantage of pv is that, it can be shared not only between pods of single node but 
>among the pods of multiple nodes. this means pv can be scaled by expanding their sizes, However reducing 
>size is not possible.

>Statefulset are the most frequent users of the pv since they need a permanant storage for their pods.


>There are two ways to provisioned the persistant volume_

1> statically  : a cluster admin creates a numbers of pv's , they carry the details of the real storage,
                 which is available for use by cluster users.

2> Dynamically : When none of the admin created pv that matches the users pvc, then the cluster 
                 may try to dynamically provisioned the volume/pv as per users request, specially for the pvc. 
                  
>                 This dynamic provisioning is based on StorageClasses, the pv must request a storage class 
>                 and the admin must have created and configured that class for dynamic provisioning to occure.

>                 to enable dynamic storage provisioning , we have to enable the DefaultStorageClass admission'
>                 controller on the k8s API server.


>-----------------------------------------------------------------------------------------------------------------

Storage Class_ :

*ref_https://containerjournal.com/topics/container-networking/using-ebs-and-efs-as-persistent-volume-in-kubernetes/

>storage class is a k8s object thats let user to specify which type of the storage they need from the 
>cloud provider. it is useful for dynamic provisioning of the PVs.

>Assume , that if the pod requires 10Gi storage capacity and no current PV can match this PVC requirement,
>the STORAGECLASS automatically creates a PV for this PVC.

>most cloud provider (managed cluster services), provides the default storage class when you set up the
>k8s cluster. 
>To check the default/available storage class within the cluster, we can get this using 
kubectl get storageclass

>different storage class represents various qualities, such as Disk speed like SSD/HDD, and Throughput,
>backup or replication policies. Type of the file system are selected depending on the scenario and the 
>need cloud provider supports.

>Minikube provides default storage class as HostPath type.

>-----------------------------------------------------------------------------------------------------------------

*from_cloud_deeptech_

>Dynamic volume provisioning using storage class.

>Storage Class :-

storage class is used to manage persistance volume. 
storage class will do Dynamic provisioning of storage using cloud provider like aws, gcp etc.

if our cluster is not associated with any cloud provider, we will not be able to have storage class and 
that our claim will fail.

storage class / pv are not namespaced object. pvc is namespaced.

>ref_ https://kubernetes.io/docs/concepts/storage/storage-classes/


apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: k8s-storage-sc

provisioner: kubernetes.io/aws-ebs     # determines what volume plugin is used for provisioning
parameters:
  type: gp2
  iopsPerGB: "10"
  fsType: ext4                         # gp2, aws specific

reclaimPolicy: Retain                  # retain or delete
allowVolumeExpansion: true             # allow dymanic resizing, can be expanded
mountOptions:
  - debug
volumeBindingMode: Immediate           # Immediate or waitForFistConsumer


>Provisioner : lets say we have our cluster deployed in OnPrem , and we wants to use aws EBS/EFS as storage 
>              using Storage class, in this case we can set up a NFS system using ebs/efs and then we can 
>              create a storage class that referes to the NFS storage. 
(most developers have abondon the practice of creating PV and now, most of the volumes are provisioned Dynamically)


>to set the default storage class, we can add the annotation in metadata as_
annotation: 
  storageClass.kubernetes.io/is-default-class: "true"


*volumeBindingMode_

>Immediate            : -  mode indicates that volume binding and dynamic provisioning occurs once the 
>                          PersistentVolumeClaim is created

>WaitForFirstConsumer : -  mode which will delay the binding and provisioning of a PersistentVolume until a
>                          Pod using the PersistentVolumeClaim is created.


*important_
>It is important to understand that, once PV is claimed by PVC, that PV will not be claimed by another PVC.
>Even if it have more than enough space available as demanded by PVC, it can not be usd by other PVC's unless
>claim on that PV is removed and proper policy is defined.



>pv :-

apiVersion: v1
kind: PersistentVolume
metadata:
  name:  k8s-storage-pv 

spec:
  capacity:
    storage: 10Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain    #Recycle or delete
  storageClassName: k8s-storage-sc



>pvc :-

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: k8s-storage-pvc

spec:       
  storageClassName: k8s-storage-sc
  volumeMode: Filesystem                       
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi

>even if we do not defined any specific storageclass in pvc manifest, it automatically refers to default 
>storage class(standard). 


>POD_

apiVersion: v1
kind: Pod
metadata:
  name: k8s-storage-pod

spec:
  containers:
  - name: k8s-storage-containers
    image: nginx

    volumeMounts:
      - name: k8s-storage-volume
        mountPath: /home/data

  volumes:
    - name: k8s-storage-volume
      persistentVolumeClaim:
        claimName: k8s-storage-pvc


*flow_
>aws ebs ---> StorageClass ---> PV ---> PVC ---> Pod/Deployments ---> Application
*----------------------------------------------------------------------------------------------------------------

*read_next:
>Persistance volume claim templates

>-----------------------------------------------------------------------------------------------------------------