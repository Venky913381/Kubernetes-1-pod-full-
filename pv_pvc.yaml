>

>What problems does it solve?
    Containers running inside the pod can not share the files with each other.
    All the files inside the container are temporary which means if you terminate the container,
    you are going to lose all your files.
    Secondly if in any case, your container crashes then there is no way to recover files.
    Kuberenetes provides volume plugin as Persistent Volume to address the above problems.

    The lifecycle of these volumes are independent of the lifecycle of pods.
    So if PODs are terminated then volumes are unmounted and detached keeping the data intact.

>What is Persistent Volume(PV)?
    In simple terms, it's storage available within your Kubernetes cluster. 
    This storage can be provisioned by you or Kubernetes administrator.

    It's basically a directory with some data in it and all the containers running inside the pods can access it. 
    But Persistent Volumes are independent of the POD life cycle.

    So if PODs live or die, persistent volume does get affected and it can be reused by some other PODs.


  >  Kubernetes provides many volume plugins based on the cloud service provider you are using ___
        awsElasticBlockStore, azureDisk, azureFile, cephfs, cinder, configMap, csi, downwardAPI, 
        emptyDir, fc (fibre channel), flexVolume, flocker, gcePersistentDisk, gitRepo (deprecated), 
        glusterfs, hostPath, iscsi, local, nfs, persistentVolumeClaim, projected, portworxVolume, 
        quobyte, rbd, scaleIO, secret, storageos, vsphereVolume


>What is Persistent Volume claim?
    Persistent volume provides you an abstraction between the consumption of the storage 
    and implementation of the storage.
    In the nutshell you can say, its a request for storage on behalf of an application which is running on cluster.


>---------------------------------------------------------------------------------------------------------------

>flow :-   POD------>PVC------->PV
1. create/execute manifest for volume i.e PV 
2. bind the volume to PVC i.e. execute manifest for pvc 
3. Create the pod 


>POD_

apiVersion: v1
kind: Pod
metadata:
  name: jhooq-pod-with-pvc
  labels:
    name: jhooq-pod-with-pvc
spec:
  containers:
  - name: jhooq-pod-with-pvc
    image: rahulwagh17/kubernetes:jhooq-k8s-springboot
    ports:
      - containerPort: 8080
        name: www

    volumeMounts:
      - name: www-persistent-storage
        mountPath: /home/vagrant/storage

  volumes:
    - name: www-persistent-storage
      persistentVolumeClaim:
        claimName: jhooq-pvc

>PVC_

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jhooq-pvc
spec:
  volumeName: jhooq-demo-pv
  storageClassName: local-storage     # must need to same as describe in PV specs, it will decide which PV 
  volumeMode: Filesystem                         # will get served the claim.
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi   # 1gib out of 10gib provisioned by pv.


>PV_

apiVersion: v1
kind: PersistentVolume
metadata:
  name: jhooq-demo-pv

spec:
  capacity:
    storage: 10Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain     #other, Delete
  storageClassName: local-storage
  local:                                    #or-->  hostPath:
    path: /home/vagrant/storage             #          path: "/path"

  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1


>kubectl get pv
>kubectl get pvc
>kubectl describe pv <pv_name>
>kubectl describe pvc <pvc_name>

>If there are multiple PV's and if we do exclusively mentioned the volumeName in pvc, 
>then k8s will check, what pv will satisfy the storage need as per pvc and automatically those will
>be bind together.


> accessModes, can be_ (from statefulset video)
     ReadWriteOnce (RWO) : only single worker node can mount the volume for reading and writing at same time.
                           this mode will still allow multiple pods to access the volume when pods are running
                           on same node.

     ReadOnlyMany (ROX)  : the volume can be mounted as read-only by many nodes at the same time
     ReadWriteMany (RWX) : the volume can be mounted as read-write by many nodes at the same time


>persistentVolumeReclaimPolicy, can be_

  >reclaim policy tell us , what happens to persistent volume , when the PVC is deleted

     Delete  : it deletes volume content and makes the voulme available to be claimed again as 
               soon as pvc gets deleted. 
     Retain  : pv content will be persisted after pvc is deleted and it can not be reused, until
               administrator reclaim the volume manually.

>--------------------------------------------------------------------------------------------------------------- 


>Pros and Cons_
  After using the persistent volume and persistent volume claim, 
  I must say its always beneficial to use both when you are working in the production environment because
  you can not just delete the data after the end of the POD cycle.

  The most favorable use case which I can see setting up the database such as
  MySQL, Oracle, Postgress inside the persistent volume so that it is always there irrespective 
  of your POD life cycle.

>Pros_
  Storing and archiving the logs
  Setting up the database
  Useful for application handling a large number of batch jobs
  Storing configs of application
  Independent from PODs life-cycle
  Easy to use
  Easy to backup

>Cons_
  Can not be used to store transnational data for performance-intensive application
  You need to set up your own backup policies
  Couldn't be used for HA(High Availability)

>Conclusion_
  If you are reading this part of the blog post then I must say you have at least 
  implemented PV and PVC inside your Kubernetes cluster. But to conclude it here is recap of what we did -

  Gone through the concepts of "What is Persistent Volume and Persistent Volume Claim"
  Then we created a Persistent Volume .i.e - jhooq-demo-pv with 1 Gi of storage
  Created the Persistent Volume Claim .i.e. - jhooq-pvc to use persistent volume jhooq-demo-pv
  Finally created the POD and deployed spring boot microservice.


>---------------------------------------------------------------------------------------------------------------

>Admin creates a pool of persistant volumes/PV's for user to choose from. pv can have varying properties, 
>such a performance for different problems.
>it is a cluster wide resource used to store /persist data beyond the lifetime of a pod.

>pv's are not backed by any locally attached storage on worker node but by networked storage system 
>such as cloud providers storage or NFS or distributed filesystem like Ceph or GlusterFS. 

>PV provide a file system that can be mounted on to the cluster, without being associated with any perticular
>node.

>it act as an abstraction layer to save the user from going in to the details of storage is managed
>and provisioned by each cloud provider. This api object captures the details of the implemantation
>of the storage , be that NFS, iSCSI or cloud provider specific storage system.

>advantage of pv is that, it can be shared not only between pods of single node but 
>among the pods of multiple nodes. this means pv can be scaled by expanding their sizes, However reducing 
>size is not possible.

>Statefulset are the most frequent users of the pv since they need a permanant storage for their pods.



>There are two ways to provisioned the persistant volume_

1> statically :- a cluster admin creates a numbers of pv's , they carry the details of the real storage,
                 which is available for use by cluster users.

2> Dynamically :- When none of the admin created pv that matches the users pvc, then the cluster 
                  may try to dynamically provisioned the volume/pv as per users request, specially for the pvc. 
                  
>                  This provisioning is based on StorageClasses, the pv must request a storage class 
>                  and the admin must have created and configured that class for dynamic provisioning to occure.

>                 to enable dynamic storage provisioning , we have to enable the DefaultStorageClass admission'
>                 controller on the k8s API server.


>some pending, check screenshots (from statefulset video)

>-----------------------------------------------------------------------------------------------------------------
