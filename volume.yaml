>
*STORAGE_IN_KUBERNETES_

*ref_https://kubernetes.io/docs/concepts/storage/volumes/

>pods are Ephemeral in nature, each container in pod will have its own isolated file system , which
>comes from containers image.
>each time container gets restarted , the file system gets reset. So any chnages done to file system 
>earlier will not be persisted, THAT IS WHY WE NEED TO USE VOLUMES to have persistant data_

>in k8s, there are some volume which gets created when the pod is started and are destroyted when
>the pod is deleted. they have lifecycle similar to that of pod. these are EPHEMERAL volumes.

>some volumes will have lifecycle beyond the lifecycle of the pod or even k8s node itself, these 
>volumes are known as persistance volumes.

>volumes is essentially a directory accessible to all the containers inside the pod.
>by default containers data is stored inside its own file system, which is ephemeral in nature.  
>in order to have persistance storage, k8s provides the volumes. 

>Persistance storage provides the data beyond the lifecycle of the pod, i.e. persistance volumes/ hostPaths
>is a storage object that lives at the cluster level and has lifecycle beyond the pod.

Kubernetes offers three type of different storage options to the user, these are :
1> EmptyDir
2> HostPath
3> PersistantDisk

>---------------------------------------------------------------------------------------------------------------

*EMPTYDIR ( Creating volume inside pod, i.e. alongside container (EmptyDir) )

*ref_https://kubernetes.io/docs/concepts/storage/volumes/#emptydir

> we specify the volumes to provide for the pod in .spec.volumes and declare where to mount them into 
> containers in .spec.containers[*].volumeMounts

> each container defined in pod must , we must independantly specify where to mount the volume in the container.

> a process in a container sees a filesystems view composed from the initial content of the container image, 
> plus any mounted volumes into it. i.e. each container can mount the same volume at different independent 
> paths of their own.

apiVersion: v1 
kind: Pod
metadata: 
  name: pod1 

spec:
  containers:
    - name: vol1 
      image: nginx 

      volumeMounts:
        - mountPath: /data 
          name: test-vol

  volumes:
    - name: test-vol
      emptyDir: {}        # emptyDir.medium,  feild to memory
      sizeLimit: 500Mi

>data will remain inside the pod and will be available only to the containers inside that pod.
>this data will not be available to other containers on other pods on same machine or other pods on other machines.

>emptyDir is a temporary storage and it have a lifecycle similar to the pod. i.e. once pod is deleted
>VOLUME and DATA will also be lost forever.

>it is initially empty, as it was created inside the pod. emptydir volumes are stored on whatever medium is 
>backing the node, that is it might be disk or n/w storage or RAM (emptyDir.medium feild to Memory)

>all containers in the pod can read and write to this volume by mounting the volume at some path in their
>file system.

>when pod is removed from node, data will be lost for ever, it is mainly used to store cache or temporary
>data to be processed.

>usage of emptyDir volume type _
  1. Scratch space, if container needed it for temp trasactional data or such as for a disk-based merge sort
  2. as a Cache
  3. checkpointing a long computation for recovery from crashes
  4. holding files that a content-manager container fetches while a webserver container serves the data

>--------------------------------------------------------------------------------------------------------

*HostPath ( Creating volume outside of pod, i.e. on Node machine. (HostPath) )

*ref_https://kubernetes.io/docs/concepts/storage/volumes/#hostpath

> we specify the volumes to provide for the pod in .spec.volumes and declare where to mount them into containers
> in .spec.containers[*].volumeMounts

> for each container defined in pod, we must independantly specify where to mount the volume in the container.

> a process in a container sees a filesystems view composed from the initial content of the container image, 
> plus any mounted volumes into it. i.e. each container can mount the same volume at different independent 
> paths of their own.

> if we delete the POD, data will be preserved in directory which is in sync on Host Machine, 
> and when new POD is recreated by controller, all the data will be present in new POD inside the container.
  .i.e.
> data volumes is created On the Host Machine storage, and will be available to all the PODs on that machine.

>it have lifecycle of the node and it can share the data between the pods

apiVersion: v1 
kind: Pod
metadata: 
  name: pod2

spec:
  containers:
    - name: vol1 
      image: nginx
      volumeMounts:
        - name: test-vol2            # on container
          mountPath: /data

  volumes:                           # volume on host machine
    - name: test-vol2                #/data,  on container & /tmp/data, on host, will be in sync.
      hostPath:
          path: /tmp/data   
          type: Directory or Create  #optional 


>this type of volume mounts a File or Directory created on node where pod is running.

>hostpath directory referes to directory created on node where pod is running.

>use it with caution becouse when pod are schedule on multiple nodes , each node gets own hostPath 
>storage volume. these may not be in sync with each other and different pods might be using a different data.

>when node become unstable, the pod might fails to access the hostpath directory and eventually gets terminated.

>life cycle of the Host Path is independant of the Pod. i.e. even if pod dies, the data/host path will remains.

> usage of HostPath _
1. as a storage for cluster running on single node.
2. running a container that needs access to Docker internals; use a hostPath of /var/lib/docker
3. running cAdvisor in a container; use a hostPath of /sys 
4. allowing a Pod to specify whether a given hostPath should exist prior to the Pod running, 
   whether it should be created, and what it should exist as

>-------------------------------------------------------------------------------------------------------------

*Volume_Mounted_on_AWS_EBS_ 
EKS ON AWS  ( Persistance data , with cluster on the cloud) (deprecated)    

*ref_https://kubernetes.io/docs/concepts/storage/volumes/#awselasticblockstore

>In above two cases, in case of pods recreation, if pod is get schedule on any machine , other than its original 
>machine in cluster data will no longer be available to the pod.

>In this all we need a volume which is available to the all the machines in the cluster, 
>Here AWS EKS (cluster in cloud) comes to rescue_ here lifecycle of the volume is not tied to the lifecycle 
>of the pod or Node. volume persist beyond the pod or node itself.

>EBS volume can be pre-populated with data, and that data can be shared between pods.

>Before you can use an EBS volume with a pod, you need to create it.
 aws ec2 create-volume --availability-zone=<az> --size=10 --volume-type=gp2

There are some restrictions when using an awsElasticBlockStore volume:

    1. the nodes on which pods are running must be AWS EC2 instances
    2. those instances need to be in the same region and availability zone as the EBS volume
    3. EBS only supports a single EC2 instance mounting a volume


apiVersion: v1
kind: Pod                                                 
metadata: 
  name: vol2
spec: 
  containers:                
    - name: container5          
      image: nginx
      imagePullPolicy: IfNotPresent
      
      volumeMounts:
        - mountPath: /data
          name: test-volume

  volumes:
    - name: test-volume
      awsElasticBlockStorage:                 # attach ebs volumes pod.
          volumeID: "vol-02c13c4470d26d461"
          fsType: ext4

>in this aproach developer needs to know the details of underline storage such as aws ebs, also if we 
>migrate our cluster to some other cloud provider, in that we need to make changes to the storage and
>again developer will needs a details of it.
>to overcome this k8s provides the PV and PVC's, which hides the details of underlying storage, all the 
>developer needs is to use it, without knowing the underlying details.

>If pod get schedule on a perticular node, then EBS volume will get attached to that node automatically and
>if we delete and recreates the pod , on another machines in cluster, then that EBS volume will automatically get
>attached to that machine. and this way pod will always have access to the volume and the data.


Kubernetes supports several different types of Volumes :
1.emptyDir
2.hostPath
3.awsElesticBlockStorage through CSI (container storage interface)
4.gcePersistanceDisk
5.NFS
6.Cephs
7.PVC
8.azureDisk
9.ConfigMaps and Secrets
and many more...........

*note_ similar to "awsElesticBlockStorage", most of the other cloud storages are now deprecated by k8s,
       and are no longer in use. (refer the link for more info)

*READ_NEXT:- 
>PV&PVC


>-------------------------------------------------------------------------------------------------------------