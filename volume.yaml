>

>pods are Ephemeral in nature, each container in pod will have its own isolated file system , which
>comes from containers image.
>each time container gets restarted , the file system gets reset. So any chnages done to file system 
>earlier will not be persisted, THAT IS WHY WE NEED TO USE VOLUMES_

>In k8s, there are some volume which gets created when  the pod is started and are destroyted when
>the pod is deleted. they have lifecycle similar to that of pod. these are EPHEMERAL volumes.

>some volumes will have lifecycle beyond the loifecycle of the pod or even k8s node itself, these 
>volumes are known as persistance volumes.

>volumes is essentially a directory accessible to all the containers inside the pod.
>by default containers data is stored inside its own file system, which is ephemeral in nature.  
>in order to have persistance storage, k8s provides the volumes. 

>Persistance storage provides the data beyond the lifecycle of the pod, i.e. persistance volumes/ hostPaths
>is a storage object that lives at the cluster level and has lifecycle beyond the pod.


Kubernetes offers three type of different storage options to the user, these are :
1> EmptyDir
2> HostPath
3> PersistantDisk


*EMPTYDIR ( Creating volume inside pod, i.e. alongside container (EmptyDir) )

> we specify the volumes to provide for the pod in .spec.voules and declare where to mount them into containers
> in .spec.containers[*].volumeMounts

> each container defined in pod must , we must independantly specify where to mount the volume in the container.

> a process in a container sees a filesystems view composed from the initial content of the container image, 
> plus any mounted volumes into it. i.e. each container can mount the same volume atr different independent 
> paths of their own.

apiVersion: v1 
kind: Pod
metadata: 
  name: pod1 

spec:
  containers:
    - name: vol1 
      image: nginx 

      volumeMounts:
        - mountPath: /data 
          name: test-vol

  volumes:
    - name: test-vol
      emptyDir: {}        # emptyDir.medium,  feild to memory


>data will remain inside the pod and will be available only to the containers inside that pod.
>this data will not be available to other containers on other pods on same machine or other pods on other machines.

>emptyDir is a temporary storage and it have a lifecycle similar to the pod. i.e. once pod is deleted
>volume and data will also be lost.

>it is initially empty, as it was created inside the pod. emptydir volumes are stored on whatever medium is 
>backing the node, that is it might be disk or n/w storage or RAM (emptyDir.medium feild to Memory)

>all containers in the pod can read and write to this volume by mounting the volume at some path in their
>file system.

>when pod is removed from node, data will be lost for ever, it is mainly used to store cache or temporary
>data to be processed.

>--------------------------------------------------------------------------------------------------------

*HostPath ( Creating volume outside of pod, i.e. on Node machine. (HostPath) )

> we specify the volumes to provide for the pod in .spec.voules and declare where to mount them into containers
> in .spec.containers[*].volumeMounts

> each container defined in pod must , we must independantly specify where to mount the volume in the container.

> a process in a container sees a filesystems view composed from the initial content of the container image, 
> plus any mounted volumes into it. i.e. each container can mount the same volume atr different independent 
> paths of their own.

>if we delete the POD, data will be preserved in directory which is in sync on Host Machine, 
>and when new POD is recreated by controller, all the data will be present in new POD inside the container.
.i.e.
>data volumes is created On the Host Machine storage, and will be available to all the PODs on that machine.

>it have lifecycle of the node and it can share the data between the pods

apiVersion: v1 
kind: Pod
metadata: 
  name: pod2

spec:
  containers:
    - name: vol1 
      image: nginx
      volumeMounts:
        - name: test-vol2            # on container
          mountPath: /data

  volumes:                           # volume on host machine
    - name: test-vol2                #/data,  on container & /tmp/data, on host, will be in sync.
      hostPath:
          path: /tmp/data   
          type: Directory or Create  #optional 


>this type of volume mounts a File or Directory created on node where pod is running.

>hostpath directory referes to directory created on node where pod is running.

>use it with caution becouse when pod are schedule on multiple nodes , each node gets own hostPath 
>storage volume. these may not be in sync with each other and different pods might be using a different data.

>when node become unstable, the pod might fails to access the hostpath directory and eventually gets terminated.

>life cycle of the Host Path is independant of the Pod. i.e. even if pod dies, the data/host path will remains.

>-------------------------------------------------------------------------------------------------------------

*Volume_Mounted_on_AWS_EBS_
EKS ON AWS  ( Persistance data , with cluster on the cloud)                             

>In above two cases, in case of recreation, if pod is get schedule on any machine , other than its original 
>machine in cluster data will no longer be available to the pod.

>In this all we need a volume which is available to the all the machines in the cluster, 
>Here AWS EKS (cluster in cloud) comes to rescue_ here lifecycle of the volume ius not tied to the lifecycle 
>of the pod or Node. volume persist beyond the pod or node itself.

>In this approch , we create a managed k8s cluster on aws eks, and create a ebs volume, which will remain
>available to the all nodes participating in the managed cloud cluster.  
(managed cluster .i.e. AWS ITSELF WILL MANAGED MASTER NODE/CONTROL PLANE)

>If pod get schedule on a perticular node, then EBS volume will get attached to that node automatically and
>if we delete and recreates the pod , on another machines in cluster, then that EBS volume will automatically get
>attached to that machine. and this way pod will always have access to the volume and the data.

>Create EKS cluster on K8s_
>1)install awscli
>2)with help of IAM user log in to aws account via aws-cli.
>3)start accessing aws services over awscli

#to create EKS cluster_
>eksctl create cluster --name <cluster-name> --node-type <type of ec2> --region <region> --node-zones <az>

>eksctl create cluster --name my-cluster --node-type t2.small --region ap-south-1 --node-zones ap-south-1a

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: vol2
spec: 
  containers:                
    - name: container5          
      image: nginx
      imagePullPolicy: IfNotPresent
      
      volumeMounts:
        - mountPath: /data
          name: test-volume

  volumes:
    - name: test-volume
      awsElasticBlockStorage:                 # attach ebs volumes pod.
          volumeID: "vol-02c13c4470d26d461"
          fsType: ext4
      

>once our cluster get set up, it will also download the kube-config in to local machine. and we will 
>able to directaly use the kubectl commands on eks, No manual configuration is needed.

>in this aproach developer needs to know the details of underline storage such as aws ebs, also if we 
>migrate our cluster to some other cloud provider, in that we need to make changes to the storage and
>again developer will needs a details of it.
>to overcome this k8s provides the PV and PVC's, whcih hides the details of underlying storage, all the 
>developer needs is to use it, without knowing the underlying details.


Kubernetes supports several different types of Volumes :
1.emptyDir
2.hostPath
3.awsElesticBlockStorage through CSI (container storage interface)
4.gcePersistanceDisk
5.NFS
6.Cephs
7.PVC
8.azureDisk
9.ConfigMaps and Secrets
and many more...........

*READ_NEXT:- 
>PV&PVC

>-------------------------------------------------------------------------------------------------------------