>

*SCHEDULING_IN_KUBERNETES_WITH_NODE-AFFINITY_

>there are various ways to schedule a pod on node or select a Node to schedule/execute the pod,
>these are_
      1.Node Name and Node Selector
      2.Node Affinity
      3.Pod Affinity and Pod-Anti-Affinity
      4.Taints and Tolerations

>kubernetes user normally do not need to choose a node to execute their pod, Instead , selection of appropriate
>Node is automatically handled by kubernetes scheduler.

>k8s schedules watches for the newly created pod that have no Node assigned and for every such pod scheduler
>becomes responsible for finding the best node for that pod to run on.

>Automatic node scheduling prevents user from scheduling pod on unhealthy / unsuitable node.
>However, k8s provide the flexibility , to allow user to select the node, so that certain condition like RAM, SSD
>is fullfilled for execution of certain pod that need such specifications.

>Node affinity is more flexible way of scheduling the pod , in this method, we define the set of rules based on 
>which scheduler schedules or DoNot Schedule the pod on the node or set of nodes.

>-----------------------------------------------------------------------------------------------------------------

# Node Affinty and Node antiAffinity_

*ref_https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/
*ref_https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
*ref_https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/
*ref_https://docs.openshift.com/container-platform/3.11/admin_guide/scheduling/index.html


>In Node Affinty , Pod will be scheduled on the proper Node based on the rules defined under
>Node affinity rules in pod spec manifest file.
>i.e based the node affinity rules, Scheduler will decides the node for the pod to get scheduled on.

>There is no dedicated clause for NODE_ANTI_AFFINITY, but we can emulate the node_anti_affinity
>behaviour using operators like "NotIn" and "DoesNotExist" . 

Node Affinity_:

>Node affinity is conceptually similar to NodeSelector or NodeName, allowing you to constrain choices of nodes , 
>the Pod can be scheduled based on NODE LABELS. However, it is more flexible and Expressive to the user 
>through the rules which we can defined. 

>In Node Affinity, Instead of directaly giving node's name or label , we give condition based on which 
>scheduler select or Unselect the node to schedule the pod. There are two types of node affinity rules/policies_

#requiredDuringSchedulingIgnoredDuringExecution: 
The scheduler can't schedule the Pod unless the rule is met / matching node is found. 
This functions like nodeSelector, but with a more expressive syntax.

#preferredDuringSchedulingIgnoredDuringExecution: 
The scheduler tries to find a node that meets the rule. If a matching node is not available, 
the scheduler still schedules the Pod.

*Note:- 
>In the preceding types, IgnoredDuringExecution means that if the node labels changes after 
>Kubernetes schedules the Pod, the Pod will continues to run.
>You can specify node affinities using the .spec.affinity.nodeAffinity field in your Pod spec.


>In "requiredDuringSchedulingIgnoredDuringExecution" , we gave the "Node Selection Terms" that node MUST 
>satisfy for the pod to get schedule on it. 

>While in  "preferredDuringSchedulingIgnoredDuringExecution" , we gave various "Preferences" for nodes 
>to be desirable for scheduling the pod.


For example, consider the following Pod spec_

apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:                              # all the terms must meet for the scheduling of pod.

        - matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: In
            values:
            - antarctica-east1     # logical OR , in values
            - antarctica-west1
                                   #LOGICAL AND, as both  matchExpressions, are under one Node selector terms.
        - matchExpressions:
          - key: instance_type     # antarctica-east1 with either t2.medium or m2.medium
            operator: In           #                         OR
            values:                # antarctica-west1 with either t2.medium or m2.medium
            - t2.medium
            - m2.medium   

      preferredDuringSchedulingIgnoredDuringExecution:
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
      - weight: 100     

  containers:
  - name: with-node-affinity
    image: registry.k8s.io/pause:2.0


>In this example, the following rules apply:

The node must have a label with the key topology.kubernetes.io/zone and the value of that label 
must be either antarctica-east1 or antarctica-west1 and another lable instance_type with values either 
t2.medium or m2.medium.

The node preferably has a label with the key another-node-label-key and 
the value another-node-label-value.

You can use the operator field to specify a logical operator for Kubernetes to use when 
interpreting the rules. You can use In, NotIn, Exists, DoesNotExist, Gt and Lt.


>NotIn and DoesNotExist allow you to define node anti-affinity behavior. 
>Alternatively, you can use node taints to repel Pods from specific nodes.

*---------------------------------------------------------------------------------------------------------------

Note: (as per doc) 
*ref_https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/

>If you specify both nodeSelector and nodeAffinity, both must be satisfied for the Pod to be scheduled onto a node.

>If you specify multiple "nodeSelectorTerms" associated with nodeAffinity types, 
>then the Pod can be scheduled onto a node if one of the specified nodeSelectorTerms can be satisfied. (LOGICAL OR)

affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:

        nodeSelectorTerms:         # term _ 1
        - matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: In
            values:
            - antarctica-east1
            - antarctica-west1
                                  # LOGICAL OR between two terms, as we defining two seperate node selector terms.
        nodeSelectorTerms:        # term _ 2
        - matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: In
            values:
            - ap-south-1
            - ap-south-2


*                                   *************************

>If you specify multiple "matchExpressions" associated with a single "nodeSelectorTerms", 
>then the Pod can be scheduled onto a node if and only if all the matchExpressions are satisfied. (LOGICAL AND)


affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:

        - matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: In
            values:
            - antarctica-east1
            - antarctica-west1
                                    #Logical AND, as both  matchExpressions, are under one Node selector terms.
        - matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: In
            values:
            - ap-south-1
            - ap-south-2
              

>in this case, there are four different possibilities for scheduler to select the node, these are the 
>possible combinations for the match xpression values_
>1.antarctica-east1 with ap-south-1
>2.antarctica-east1 with ap-south-2
>3.antarctica-west1 with ap-south-1
>4.antarctica-west1 with ap-south-2

*-----------------------------------------------------------------------------------------------------------------


Node affinity weight :

You can specify a weight between 1 and 100 for each instance of the 
>"preferredDuringSchedulingIgnoredDuringExecution" affinity type. 

When the scheduler finds the multiple nodes that meet all the other scheduling requirements of the Pod, 
the scheduler iterates through every preferred rule that the node satisfies and adds the 
value of the weight for that expression to a sum.

The final sum is added to the score of other priority functions for the node. 
Nodes with the highest total score are prioritized when the scheduler makes a scheduling decision for the Pod.


*note: 
>If you specify multiple nodeSelectorTerms associated with nodeAffinity types, 
>then the Pod can be scheduled onto a node if one of the specified nodeSelectorTerms can satisfied. (LOGICAL OR)

>If you specify multiple "matchExpressions" associated with a single "nodeSelectorTerms", 
>then the Pod can be scheduled onto a node only if all the matchExpressions are satisfied. (LOGICAL AND)


apiVersion: v1
kind: Pod
metadata:
  name: with-affinity-anti-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:                      # nodeSelectorTerms:
          - key: kubernetes.io/os      #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
            operator: In               #then the Pod can be scheduled onto a node if one of the specified
            values:                    #nodeSelectorTerms can be satisfied. (LOGICAL OR)
            - linux
            
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:                     #preference, only for iterating preferences, and determining the weight.
          matchExpressions:
          - key: label-1
            operator: In                   
            values:
            - key-1
                                 #AND
      - weight: 50
        preference:
          matchExpressions:
          - key: label-2
            operator: In
            values:
            - key-2
  containers:
  - name: with-node-affinity
    image: registry.k8s.io/pause:2.0


>After satisfying the node affinity clous of "requiredDuringSchedulingIgnoredDuringExecution" ,
If there are two possible nodes that match the "preferredDuringSchedulingIgnoredDuringExecution" rule, 
one with the label-1:key-1 label and another with the label-2:key-2 label, the scheduler considers
the weight of each node and adds the weight to the other scores for that node, 
and schedules the Pod onto the node with the highest final score.

>Note: If you want Kubernetes to successfully schedule the Pods in this example, 
>you must have existing nodes with the kubernetes.io/os=linux label.

>-----------------------------------------------------------------------------------------------------------------


Node AntiAffinity_ :

In kubernetes, there is NO exclusive policy such as a Node AntiAffinity, However, it is possible to emulate 
the behaviour , which works opposite to Node Affinity rules.


>Operator value of, "NotIn" and "DoesNotExist" allow you to define node anti-affinity behavior. 
>Alternatively, you can use node taints to repel Pods from specific nodes.

affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: NotIn
            values:
            - antarctica-east1      #these are AZ's of the node in aws cloud.
            - antarctica-west1

>above rule specifies that, pod must NOT get schedule on the Node having key as topology.kubernetes.io/zone 
>having its values as antarctica-east1 OR antarctica-west1, Which emulates the Node-Anti-Affinity
>behaviour.
i.e. key "topology.kubernetes.io/zone" is allowed, but its values must not be either of "antarctica-east1"
OR "antarctica-west1", Some Other values for key are permitted.


affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:

        - matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: DoesNotExist
            values:
            - antarctica-east1
            - antarctica-west1            


>above rule specifies that, Pod must NOT get schedule on the Node having key as topology.kubernetes.io/zone,
>in the AZ , that does not exist (key) in the values (az) as antarctica-east1 and antarctica-west1. 
i.e. 
>Any Node having key as topology.kubernetes.io/zone ,that DOES NOT EXIST in AZ having values as antarctica-east1 
>and antarctica-west1, pod will get schedule on that node. This emulates the anti Node Affinity behaviour.

*-----------------------------------------------------------------------------------------------------------------