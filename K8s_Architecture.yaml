> 

*Kubernetes_Architecture :

>Why We Need Container Orchastration_

Container Orchastration automates the --> deployment, management, scalling, LB, Redundancy , resource allocation
and networking of the containers across the cluster spread across several host , having large physical seperation
between them. it is focused on the managment of the life cycle of containers.

Enterprises that needs to deploy and manage hundreds or thousands of containers and hosts can benefit
from container orchastration. as they do Not need to manage each & every container independently, all container 
can be managed in bulk using orchastration engine like Docker-Swarm or Kubernetes.

k8s is open source container management tool, it provides a container runtime, container orchastration,
container centric infrastructure orchastration, self healing mechanism, service discovery, load balancing,
and container scalling. it is developed in GO languange by Google.


Container orchastration is used to automate the following tasks_:

>1.configuring and scheduling the containers
>2.provisioning and deployments of the containers
>3.to manage redundancy and availability of the containers
>4.load-balacing and Autoscaling, i.e. scalling up and down to spread the load evenly across host infrastructure
>5.movement of containers from one host to another, if there is a shortage of compute resources in a host
>6.allocation of the host resources between containers
>7.external exposure of services running in a container with the outside world .i.e networking of containers
>8.load balancing and service discovery between containers
>9.health monitoring , logging of containers and hosts.


Certified Kubernetes Distribution_:

>1.Cloud Managed  :- EKS, AKS, GKE, CIVO, PLATFORM-9, 'KOPS is a utility to manage cluster on cloud' ...etc
>2.Self Managed   :- OpenShift by RedHat and Docker Enterprise
>3.Local Dev/Test :- Micro_k8s by canonical, minikube, kind
>4.Vanilla K8s    :- the core k8s project(BareMetal), Kubeadm, kubespreay, kube-sphere
>5.SpecialBuilds  :- K3S by Rancher, is a light weigt k8s distribution.


>k8s alternatives_

>1. Container as a service             : ecs, fargate, azure container instance... etc
>2. managed k8s service                : eks, aks, gke, civo, platform9, OpenShift... etc (paas offerings)
>3. paas using k8s                     : openshift, rancher offer k8s as paas
>4. lightweight container orchastrator : docker-swarm, hashicorp Nomad, apache mesos.. etc


>---------------------------------------------------------------------------------------------------------

Master Node : 

>master node is responsible for managing the entire cluster. we can access the master node via CLI, 
>GUI or API. This is the machine on which Kubernetes is installed.

>The master watches over the nodes in cluster and is responsible for the actual orchastration of the 
>containers on the worker nodes.

>for achieving the fault tolerance there can be more than one master node in the cluster.

>it is the access point from which the user interacts with the cluster to manage the scheduling and 
>deployments of the containers.

>it has four componets , API SERVER, ETCD, SCHEDULER and CONTROLLER. All together known as Control Plane.

>Even If master is failed in any way, Workload will remain persistant on worker node. However, it will
>not be possible to communicate and control the cluster, as all communication and control of workloads and pods
>happens over the master node. master node is responsible for dispatching the workloads to the worker nodes.

>we can communicate with master node through its frontend component, API-SERVER by using kubectl CLI utility,
>or GUI utility like Kube-Lens, K8S Dashboard or an API http request via API endpoint.

*ref_https://kubernetes.io/docs/concepts/overview/components/

*Master_Node_Components _

API-SERVER: 
>It acts as Front-end for the kubernetes cluster, Kubectl cli utility talks only with api server. To communicate
>with other kubernetes manager components we have to pass through the api-server.

>The Kubernetes API server Validates and Configures data for the api objects which include pods, services, 
>replication controllers and others. The API Server services REST API operations and provides the frontend to 
>the cluster's shared state through which all other components interact.

>Kube API Server in k8s , is tasked with communicating with other kubernetes componetes in 
>Master and passing the commands/instruction from kubectl to them to complete the given task.

>Kube API server Validates and Executes the REST commands. it also makes sures that Configurations in the  
>ETCD matches with current configurations of the containers deployed in the cluster

*Note_:
>Pod is nothing but the abstraction, like a BOX, which abstarct the container or multiple containers inside it,
>it is the Responsibility of KUBE_APT-SERVER to create a pod and containers inside the pod is created by CRE 
>which may be the Docker or ContainerD or any other Container Runtime.


Controller_manager:
>Controller Manager, is the brain behind the orchastration and is responsible for maintaining the desired state 
>of the cluster. it is tasked with checking the health of pods in cluster and replacing them when needed.

>it is responsible for Noticing and Responding when nodes, containers or endpoints goes down. the controller
>makes decision to bring up new containers in such cases.

>There are various controllers like_
NODE CONTROLLER, REPLICATION CONTROLLER, ENDPOINT CONTROLLER, JOB CONTROLLER, DEAMON SET CONTROLLER etc...

>Controller Manager is the unified process of the above Controllers.


Scheduler:
>Scheduler, take care of the responsibilty of resource allocation, scheduling the pod on the correct node
>which satisfies the resource need/constraints of the pod. Scheduler look for the pods resource requirement 
>and schedule them on the correct node.

>scheduler is also responsible for the distribution of the workloads across the multiple nodes. scheduling 
>decision includes factors such as Pods resource requirement, H/W-S/W policy requirements, Affinity and 
>Anti-Affinity rules, taints and Tolerations etc.


ETCD:
>ETCD, is a database of the key-value types in kubernetes. it stores the complete cluster information. All the
>desire state information is stored in ETCD. also it store all cluster information, so if cluster goes
>down , restoring cluster to previous state based on values stored in ETCD is possible.

>All the Data written in ETCD is Encrypted by default when written. In some managed k8s like Openshift
>ETCD is not encrypted by default. and in some only values are encrypted, key are not.

 
*                               ***************************************

Worker Node: 

>worker node is responsible for the execution of the tasks dispatched by the Master node. 
>It is the actual component in k8s cluster which does the job of execution of the Pods and running the
>containers. 
>It contains the Container Runtime Engine such as Docker or rocket, to spin up the container on the direction
>provided by the Kubernetes master node.
>It is  not needed to install kubernetes on worker node, only Container Runtime like Docker is needed 
>to be install on worker node.

>It is also important to node that, Kubernetes is completely independant from the Container Runtime Engine,
>so we can use any container runtime engine on the node and still control it with kubernetes.

*Worker_Node_Components _

Kubelet:
>Kubelet, is a componets in the Worker Node, it is tasked with reporting the workloads, health of the
>pods it running,  back to the Master Node. It keep checking with master node, wheather it has any more
>workloads/pods needed to run. Master Notifies the Kubelet , if it has to run the new pod or not.

Kube_Proxy:
>Kube-Proxy is a Daemon-set which runs on a each node in the cluster.
>Kube-proxy is a network proxy that runs on each node in cluster, it implements part of the k8s service concept.
>it routes the traffic coming into node from service and forward the requests to correct pod on the node.

>KubeProxy, will resolve or creates IP-table rules. these routing rules is applied by the KubeProxy on a
>specific node, so the pod to pod communication via service is possible becouse of the rules created
>by the KubeProxy component.


*                             ***************************************


#kuberntes CLI interface_

kubectl:

>Kubectl is command line utility to communicate with the Kubernetes API server in master node.
>API server exposes k8s api and serves like the frontend for the k8s control plane.

>kubectl converts the imperative commands or declaratives manifests instruction in to API calls to 
>communicate with Kube API server in Kubernetes. 

>Kube API Server in k8s , is tasked with communicating with other kubernetes componetes in 
>Master and passing the commands/instruction from kubectl to them to complete the given task.

>Kube API validates and executes the REST commands provided by the kubectl. 
>it also makes sures that Configurations in the ETCD matches with configuratins of the containers 
>deployed in the cluster. 

>Every kubenetes operation is exposed as an API endpoint and can be executed by an http request to the endpoint.

>kubectl uses these api's to interact with the cluster. and can deploy and manages application on cluster.

*-----------------------------------------------------------------------------------------------------------


Container Runtime Environment [CRE]:

>It is underlaying software that is used to run the containers and to supports the containerization 
>like Docker. but there are also other options such as_ Docker, rkt, CRI-O, ContainerD.

>Kubernetes is completely decoupled from specific container runtimes. 
>The container runtime interface (CRI) enables it, by providing the interface for container engine to couple with k8s.

>CRE , is reponsible for creating and maintaining the isolated workspaces for the containers.
>it creates a abstraction like Namespaces and Cgroups , which maintains the isolation 
>between various objects.
(Namespaces, decides what to use and Cgroups, decides how much to use)

>CRE is the component that runs on the worker Nodes and responsible for Spining up the Containers and Running it.


>-----------------------------------------------------------------------------------------------------------

Container Runtime Interface [CRI]:

*ref_https://www.aquasec.com/cloud-native-academy/container-security/container-runtime-interface/
*ref_https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/


>The CRI is a plugin interface which enables the kubelet to use a wide variety of container runtimes, 
>without having a need to recompile the cluster components

>You need a working container runtime on each Node in your cluster, so that the kubelet can launch Pods 
>and their containers.

>The Container Runtime Interface (CRI) is the main protocol for the communication between the kubelet 
>and Container Runtime.

>The Kubernetes Container Runtime Interface (CRI) defines the main gRPC protocol for the communication 
>between the cluster components, kubelet and container runtime.

>So , it is like a set of the rules, that all the CRE provider must follow in order to communicates
>with K8S API server via Kubelet. it acts as a abstraction layer to avoid large codebase for the kubelet.

>to implement the CRI, CRE must be complient with the OPEN CONTAINER INITIATIVE [OCI]. 
>OCI sets up the rules for CRI, once CRE follows all the rules ser by OCI it can used with a CRI.

>CRE, which do not implemets the CRI, need a extra layer of protocols in order to be able to communicate 
>with kubelet. Docker do not implements the CRI, so its needs a extra layer called as a DockerShim (deprecated)/
>CRI-DOCKERD to communicated with kubectl.

#with docker
>           CRI   
*KUBELET---------->DOCKESHIM---->DOCKER------>CONTAINERD------->container/container/........

#without Docker/conteinerD
>           CRI  
*KUBELET---------->CRI_CONTAINERD--------->container/container/........

>-----------------------------------------------------------------------------------------------------------

Container Networking Interface [CNI]: 

*ref_https://jvns.ca/blog/2017/10/10/operating-a-kubernetes-network/
*ref_https://stackoverflow.com/questions/53534553/kubernetes-cni-vs-kube-proxy


# pod to pod communication across the hosts

>CNI Plugin is focuses on building up an overlay network, without which Pods can't communicate with each other. 
>The task of the CNI plugin is to assign IP to the POD, when it gets scheduled. and to build a virtual device 
>for this IP, and make this IP accessable from every node of the cluster.

*Note : when a pod is get created a new container called as a Pause container container will get created inside 
        the Pod. it is not the pod or main application container that gets the ip address, It is the pause container
        which is alloated with an IP address. 
        if we deletes the pause container, Pod will self destruct and Recreates itself and new ip address is 
        gets alloted to it. we can check the pause container using docker commands.


# OVERLAY NETWORK

Kubernetes assumes that every pod has an IP address and that you can communicate with services inside that 
pod by using that IP address.

>“overlay network” is the system that lets you refer to a pod by its IP address.
>All other Kubernetes networking stuff relies on the overlay networking for functioning correctly.

>There are a lot of overlay network backends (calico, flannel, weave) and the landscape is pretty confusing.

>Overlay network has 2 responsibilities_
  1.Make sure your pods can send network requests outside your cluster.
  2.Keep a stable mapping of nodes to subnets and keep every node in your cluster updated with that mapping. 
    Do the right thing when nodes are added & removed.


# KUBE-PROXY

kube-proxy runs as a Daemon-Set on each node in the cluster.

Just to understand kube-proxy, Here's how Kubernetes services work, A service is a collection of pods, 
which each have their own IP address (like 10.1.0.3, 10.2.3.5, 10.3.5.6)

Every Kubernetes service gets an IP address (like 10.23.1.2), kube-dns resolves Kubernetes service DNS names 
to service IP addresses_ (so my-svc.my-namespace.svc.cluster.local will map/resolves to 10.23.1.2)

kube-proxy sets up iptables rules in order to do random load balancing between them.
So when you make a request to my-svc.my-namespace.svc.cluster.local, it resolves to service ip 10.23.1.2, and 
then iptables rules on your local host (generated by kube-proxy) redirect it to one of 10.1.0.3 or 10.2.3.5
or 10.3.5.6 at random doing Load Balancing.

*_fqdn------>kube_dns------>service_ip----->kube-proxy----->ip_table_rules------>LB(pod1/pod2/pod3/...)

>kube-proxy's job is rather simple, it just redirect requests from Cluster IP to Pod IP.
kube-proxy has two mode, IPVS and iptables. If your kube-proxy is working on IPVS mode, you can see the redirect 
rules created by kube-proxy by running the command [ipvsadm -Ln] on any node in the cluster.


>In short, CNI sets up the Overlay Network and Assign IP's to the Pods and OVERLAY networks define the 
>underlying network which can be used for communicating the various component of kubernetes. 
>While KUBE-PROXY is a tool to generate the IP tables rules , which let you connect to any of the pod(using servics) 
>in kubernetes no matter on which node that pod exist.


>-----------------------------------------------------------------------------------------------------------

Docker-Shim: 

*ref_https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/

>this is deprecated component of kubernetes.  Dockershim has been removed as of Kubernetes v1.24 release.
>with deprecation of docker as runtime for k8s, Docker-shim is also deprecated.

>As kubectl interacts and understands only CRI, which in turn communicates with CRI-O. 
>CRI was not able to communicates with Docker over gRPC , in order to 
>bridge the gap between CRI and Docker , kubernetes introduced the componets called as Docker-shim.

>docker-shim communicates with Docker over the REST-API.

>However, as there are many container runtimes available which works according to the CRI implementations,
>it was not feasible for k8s to maintain seperate code/support for DockerShim to support Docker as a Runtime.

>Mirantis and Docker maintains the DockerShim as standalone outside the kubernetes as a conformant interface 
>for the Docker CRE, and they called its as cri-dockerd. 
>So if we wants to use Docker as a CRE in our k8s cluster, we need to seperatly implement the cri-dockerd to
>integrate docker with kubernetes cluster.
>Docker is not just a CRE, it is bundle of the tools sits on the top of the CONTAINERD CRE. 

>-----------------------------------------------------------------------------------------------------------

Container Storage Interface [CSI] :
the component that mounts volumes in your containers


>-----------------------------------------------------------------------------------------------------------

*Kube-Scheduler_

>Kubernetes comes with a default scheduler called Kube-Scheduler, it runs as a Static Pod on the Master Node.
>The kube scheduler plays a key role to choose the right node for each pod by taking care of all the
>scheduling conditions such as Resource Requirement, Taints and Tolerations, Affinity Rules and Node Selectors
>in pods Configuration.

>when we send the request to create a pod either through Imperative approch or through Declarative approch,
>the request is accepted by API-server from kubectl, which then gets forwarded to the ETCD, and the request 
>parameter is first gets written in to the ETCD database as a State. 
>It is the Job of controller Manager to instruct the creation of Pod as per data/state written inside the ETCD, 
>But pod created first goes in to Pending State , as No Node have been assigned to create the pod. 
>It is the job of the Scheduler to assign the node, once Scheduler assigns the Node as per the Constraints, 
>pod created by Controller Manager goes from Pending state to the Running Mode.


*How_to_Schedule_the_Pod_?

*ref_https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/
*ref_https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
*ref_https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/
*ref_https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/
*ref_https://kubernetes.io/docs/reference/command-line-tools-reference/

>Scheduler does the job of scheduling the pod on the perticular node based upon the rules defined
>in various forms like Affinity and AntiAffinity Rules, Taint and Toleration , Node Name, Node Selector
>attached to the Pod, Availaibility of the H/W resources with node. 

>But apart from this there are various other factor that Scheduler considers while scheduling the pod on 
>perticular node in the cluster, This implies that even if certain node have the most resource available 
>with it and which satisfied all the scheduling conditions for the pod_

>Scheduler will still considers the different predicates/conditions while scheduling the pod,
>There are various predicates that scheduler had, Which it use to make cheduling decision, these are _

1.QUEUE
*2.FILTERS
*3.SCORE
4.NOTIFIER
5.BINDING POLICIES
6.BINDINGS


>Step 1:- Filtering the nodes
In this step, Kube-scheduler first filters out all the nodes that are suitable according to the pod's 
requirements and makes a List. If the list is empty it means that no node is capable of scheduling that pod 
and in such condition, the pod remains in a pending and unscheduled state. But if the list is not empty it 
means there are some nodes available that are capable enough to schedule the pods.

>Step 2:- Scoring the nodes
In this step, Kube-scheduler assigns a score to each node present in the list to choose the most suitable 
node for the pod and the pod gets scheduled in the node with the highest score. If there is more than one 
node with equal scores, Kube-scheduler selects one of these at random.


The various "FILTERS" in scheduler are :
01.PodFitsHostPorts
02.PodFitsHost
03.PodFitsResource
04.PodMatchNodeSelector
05.NoVolumeZoneConflict
06.NoDiskConflict
07.MaxCSIVolumeCount
08.CheckNodeMemPressure
09.CheckNodePIDPressure
10.CheckNodeDiskPressure
11.CheckNodeCondition
12.PodToleratesNodeTaints
13.CheckVolumeBindings


The various "SCORE" in scheduler are :
01.SelectorSpreadPriority
02.InterPodAffinityPriority
03.LeastRequestedPriority
04.MostRequestedPriority
05.RequestedtoCapacityRatioPriority
06.BalanceResourceAllocation
07.NodePreferAvoidPodsPriority
08.NodeAffinityPriority
09.TaintTolerationPriority
10.ImageLocalityPriority
11.ServiceSpreadingPriority
12.EqualPriority
13.EvenPodSpreadPriority


>All the above scheduling predicates decides on which node the pod will get schedule, various Node selection
>cluases in kubernetes have provided us are the ways to influence the scheduling decisions, and to 
>customise scheduling behaviour of the scheduler , these are_

1.NodeSelector
2.NodeName
3.NodeAffinity
4.PodAffinity / PodAntiAffinity
5.Taint and Tolerations
6.Topology Constraints
7.Scheduler Profiles
8.Scheduler Policies (v1.25)  (profile and policies as defined in custom scheduler)


>-----------------------------------------------------------------------------------------------------------


*Kubernetes_Components__

  master node :
    api server
    etcd
    controller manager
    scheduler

  worker node :
    kubelet
    kube-proxy

  Runtime-Networking-CLI Components:
    Kubectl
    CNI 
    CRE
    CRI 

*******************************************************************************************************************

>service and kube-proxy

service does exist in etcd entries only, it is not the process running on the any node.

so when , we communicates with any pods ip, it will not goes directaly to the service, first it referes to the 
ip table , and replaces the service ip with pods ip, belonging to that service.
and now pod can communicate with other pod using cluster n/w
Ip tables contains a reference , which states that, if any pod refers to ip of the pod, request must redirects to the
any of the pod belonging to that service.

two types of data states are stored in k8s :

>1.control-plane + etcd
>2.kube-proxy state (on nodes/data plane)

kube proxy keeps on checking for any changes in the control plane , such as addition of new service or Pod, 
if it found, new service with its endpoint, it will notifies these changes, and it will add these 
changes in to its kube-proxy state and this is how it will redirects the requests comming for pod from service 
to the pod.

check iptable rules using_
>iptable -t nat --list | wc -l
>iptable --list | wc -l


*******************************************************************************************************************

>Why does Kubernetes have 3 master nodes for HA ? Why not 2 nodes ?

Having multiple master nodes ensures that services(control plane) remain available should master node(s) fail. 
In order to facilitate availability of master services, they should be deployed with 
odd numbers (e.g. 3,5,7,9 etc.) so quorum (master node majority) can be maintained should one or 
more masters fail. In the HA scenario, Kubernetes will maintain a copy of the etcd databases on 
each master, but hold elections for the control plane function leaders kube-controller-manager 
and kube-scheduler to avoid conflicts. The worker nodes can communicate with any master's 
API server through a load balancer. Deploying with multiple masters is the minimum recommended 
configuration for most production clusters.