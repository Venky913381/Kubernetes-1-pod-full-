>
>--------------------------------------BASIC-K8S-CLI-INSTRUCTIONS-----------------------------------------------

>kubectl get <type>  
>kubectl get <type> <name>
>kubectl get <type> <name> -o wide/yaml/json > foo.yaml/yoo.json
>kubectl get <type> <name> --show-labels / -o wide   --->to get types as per the label assigned
>kubectl get all -n <name_space>                     --->to get all the resources in given namespace
>kubectl get all/<resource> --all-namespaces         --->to get all the resources in all namespaces

>kubectl logs <pod_name>                              ---> to get logs of the k8s pod
>kubectl logs -f <pod_name>                           ---> to get logs of the k8s pod in follow mode
>kubectl logs -f <pod_name> -c <container_name>       ---> to get logs of the container inside k8s pod
>kubectl logs -f <pod_name> --all-containers          ---> -f for follow/continuos logs

>kubectl exec <podname> -c <container_name> -it -- <command>   ---> to execute into running pod/container

>kubectl run <podname> --image=<image:version>     ---> Imperative command
>kubectl create -f <manifest.yaml>                 ---> Imperative object configuration
>kubectl edit <type> <name>                        ---> imperative object configuration

>kubectl apply -f <manifest.yaml>                     ---> declarative object configuration
>kubectl apply -f ./my1.yaml -f ./my2.yaml            ---> to apply multiple yaml files
>kubectl apply -f ./ or < relative path>              ---> to apply all yaml manifests in folder
>kubectl apply -f <manifest.yaml> --dry-run=client    ---> to test/dryrun the current configurations

>kubectl explain <k8s resource>              ---> to get basic manifest component details
>kubectl explain --recursive <type> | less   ---> to get all resource attributes for writting manifest
>kubectl explain <type>.<section_in_yaml>    ---> to get all resource attributes for writting manifest

>kubectl describe <type> <name>              ---> to get more details on the k8s object

>kubectl label <type> <name> label1=value1              ---> to label the existing pod
>kubectl label --override <type> <name> label1=value2   ---> to override the existing pod label
>kubectl label <type> --all label1=value1               ---> to label all <type> resources in one go
>kubectl label <type> <name> label1-                    ---> (-) to delete the lable

>kubectl delete <type> <name>
>kubectl delete -f <manifest.yaml>    ---> will delete all the resources declared in manifest file.
>kubectl delete -f ./path             ---> will delete all the resources declared in manifest folder.
>kubectl delete all --all             ---> will delete all k8s resources running

>kubectl get nodes -o wide    ---> to get details on the node participating in cluster
>kubectl cluster-info         ---> gives the cluster details
>kubectl api-resources        ---> gives the lost of resources, short names, namespace support,version, kind etc.
>kubectl config view          ---> to view the configuration file. (--minify, to minify o/p)
>kubectl config get-clusters  ---> (--help, for more options)

>kubectl api-resources --namespaced=true/false  ---> to filter the objects supporting namespace.
>kubectl top nodes/pods                         ---> to get resource usage matrices (need matrics server installed)
>kubectl api-versions                           ---> to get api version of k8s objects
>kubectl get componentstatuses/cs               ---> to get the status of components of cluster
>kubectl run <type> --help                      ---> to get Imperial commmand structure for k8s resource
>kubectl -n kube-system top nodes

>minikube --help           ---> for minikube commands.
>minikube start --force    ---> to start minikube on booted/restarted machine
>minikube status           ---> Gets the status of a local Kubernetes cluster
>minikube stop             ---> Stops a running local Kubernetes cluster
>minikube delete           ---> Deletes a local Kubernetes cluster
>minikube dashboard        ---> Access the Kubernetes dashboard running within the minikube cluster
>minikube pause            ---> pause Kubernetes
>minikube unpause          ---> unpause Kubernetes
>minikube ip               ---> to get cluster ip

>minikube addons list                              ---> to get list of addons comes with minikube
>minikube addons enable/disable <addon_name>       ---> to enable/disable addons

>minikube start --container-runtime='runtime_name'/--drive='driver'    ---> if other than default docker
>minikube service  <service name> --url

#---------------------------------------------------------------------------------------------------------------


>Why We Need Container Orchastration_

Container Orchastration automates the deployment, management, scalling, and networking of the 
containers across the cluster.it is focused on the managing the life cycle of container.

Enterprises that need to deploy and manage hundreds or thousands of containers and hosts can benefits
from container orchastration. as they do need to manage each every container independently, all container 
can be managed in bulk using orchastration engine like Docker-Swarm or Kubernetes.

k8s is open source container management tool, it provides a container runtime, container orchastration,
container centric infrastructure orchastration, self healing mechanism, service discovery, load balancing,
and container scalling. it is wriitten in go languange.

>Container orchastration is used to automate the following tasks_

>1.configuring and scheduling of the containers
>2.provisioning and deployments of the containers
>3.redundancy and availability of the containers
>4.scalling up and down to spread the load evenly across host infrastructure
>5.movement of containers from one host to another , if there is a shortage of compute resources in a host.
>6.allocation of the resources between containers
>7.external exposure of services running in a container with the outside world
>8.load balancing of service discovery between contrainer
>9.health monitoring of containers and hosts.


>Certified Kubernetes Distribution_

>1.Cloud Managed  :- EKS, AKS, GKE, CIVO, PLATFORM-9, 'KOPS is a utility to manage cluster on cloud' ...etc
>2.Self Managed   :- OpenShift by RedHat and Docker Enterprise
>3.Local Dev/Test :- Micro k8s by canonical, minikube, kind
>4.Vanilla K8s    :- the core k8s project(BareMetal), Kubeadm, kubespreay, kube-sphere
>5.SpecialBuilds  :- K3S by Rancher, is a light weigt k8s distribution.


>k8s alternatives_

>1. Container as a service             : ecs, fargate, azure container instance... etc
>2. managed k8s service                : eks, aks, gke, civo, platform9,OpenShift... etc (paas)
>3. paas using k8s                     : openshift, rancher offer k8s as paas
>4. lightweight container orchastrator : docker swarm, hashicorp Nomad, apache mesos.. etc


>Kubectl is command line utility to communicate with the Kubernetes API server.
>kubectl converts the imperative commands/manifest instruction in to API calls to communicate with
>Kube API server in Kubenetes. Kube API Server in k8s , is tasked with communicating with other 
>kubernetes componetes in Master and passing the commands/instruction to them to complete the given task.

>It is the responsibily of Controller Manager, to maintain the desired state of the cluster. it is tasked
>with checking the health of pods in cluster and replacing them when needed.

>Scheduler, take care of the responsibilty of resource allocation, scheduling the pod on the correct node
>which satisfies the resource need of the pod. Scheduler look for the pods resource requirement and schedule
>them on the correct node.

>ETCD, is a database of key-value types in kubernetes. it stores the complete cluster information. All the
>desire state information is stored in ETCD. also it store all cluster information, so if cluster goes
>down , restoring cluster to previous state based on values stored in ETCD is possible.

>Kubelet, is a componets in the Worker Node, it is tasked with reporting the workloads, health of the Pod
>worker it running back to the Master Node. It keep checking with master node, wheather it has any more
>workloads/pods to run. Master Notifies the Kubelet , if it has to run the new pod or not.

>KubeProxy, will resolve or creates IP table rules. these routing rules is applied by the KubeProxy in
>specific node, so the pod to pod comminucation via service is possible becouse of the rules created
>by the KubeProxy component.


#----------------------------------------------------POD---------------------------------------------------------

>read : https://kubernetes.io/docs/concepts/workloads/pods/

>kubectl run <podname> label=value --image=<image:version> -it --rm --restart=Never -- <cmd> 

> --image   : specifies image
> -it       : interactive mode
> -rm       : specifies, to remove pod, when it is stopped
> --restart : specifies restart policy for container
> --port=<> : specifies the port to be open on container
> --expose  : this will create the service for the pod
> --replicas: to create multiple pod replicas

>kubectl run pod --help
>kubectl explain pod.spec / pod.metadata


apiVersion: v1
kind: Pod 

metadata: 
  name: firstpod
  labels: 
     env: prod
     name: swapnil
  namespace: test-space   
spec: 
  containers:
    - name: firstcontainer
      image: coolgourav147/nginx-custom
      env:                                  >  env variables are key:value pairs wriitten as array.
        - name: owner
          value: swapnil
        - name: city 
          value: pune 
      args: ["sleep","3600"]  
  initContainers:
    - name: init1container
      image: coolgourav147/nginx-custom
      env: 
        - name: name
          value: swapnil
          name: city 
          value: nagpur
      args: ["sleep","15"] 

    - name: init2container
      image: coolgourav147/nginx-custom
      env: 
        - name: name
          value: swapnil
          name: city 
          value: nagpur
      args: ["sleep","15"]

>kubectl apply -f <filename.yaml>  / --dry-run=client
>kubectl get pod / -o wide / --show-labels / -w
>kubectl exec <podname> -c <containerName> -it -- <command>   ---> executing command inside the container.

#------------------------------------------service---------------------------------------------------

read more on services: https://kubernetes.io/docs/concepts/services-networking/service/

>service is k8s resource used to expose the pod/containers for it to accept the outside/internet traffic. 
>Using service we open the port on the pod.

>kubectl explain service.spec / service.metadata

>NodePort service

apiVersion: v1
kind: Service

metadata:
      name: firstservice
      labels:
        servicelabel: label1
spec:
      type: NodePort
      selector:
          type: app
      port:
        - nodePort: 32000    #port on the node/machine 
          port: 9000         #port on the pod/port opened by service
          targetPort: 80     #port opened on the container inside pod/this is end point of the service.
    
>

 #--------------------------------POD AND SERVICE------------------

apiVersion: v1
kind: Pod
metadata: 
  name: pod1 
  labels: 
     type: app   #this label is use by selector of service
spec:
  containers:
    - image:  coolgourav147/nginx-custom
      name:  firstcontainer
      ports: 
      - containerPort: 80
      - containerPort: 3535
      - containerPort: 2525




apiVersion: v1                
kind: Service
metadata:
      name: service1
      labels:                                            ip:32000---->9000---->80
        servicelabel: label1
spec:
      type: NodePort
      port:
        - nodePort: 32000  
          port: 9000      
          targetPort: 80
      selector:
          type: app  # this must be same as pods label, key-value pair

>-----------------------------------------------------------------------------

>1) http://ip:31000 ---->9000---->3200 --->app (selector)
>2) http://ip:31000 ---->9000---->my-service  --->app (selector)


  selector:
          type: app
  ports:                                               
  - nodePort: 31000
    port: 9000      
    targetPort: 3200  

    name: my-service _ read below
    protocol: tcp

> for using "name" container specs must mentioned the Port attributes with containerport and its name in container
> spec as below.....
ports:
- containerPort: 3200        
  name: my-service  



>kubectl expose pod <podname> --type=<NodePort/ClusterIP/lb> --port=<port> --target-port=<port> --name <svcName>
>kubectl apply -f <filename.yaml>  / --dry-run=client
>kubectl get service/svc -o wide / --show-labels


>kubectl port-forward --address 0.0.0.0 svc/<serviceName> <OnHost>:<OnPod>
>kubectl port-forward --address 0.0.0.0 svc/<Service-Name> --namespace <NameSpace Name> <Machine/HostPort:PodPort>


if minikube is running on aws ec2:

>You need to port forward the traffic from your EC2 node to minikube, as minikube itself will
>runs as separate Virtual Machine on ec2.
Once you have kubectl setup on the host machine, talking to the minikube cluster, 
you can use kubectl port-forward to forward traffic to any service/pod running inside minikube.

>kubectl port-forward --address 0.0.0.0 svc/<svc-name> <host-port>:<service-port>
>You should be able to access your app at IP:<host-port> as long as the port-forwarding is set up.


#-----------------------------------------ReplicationController-----------------------------------------------

>read: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/

>USES ONLY EQUILITY BASED SELECTOR.

apiVersion: v1                      
kind: ReplicationController

metadata:
    name: firstrc
    labels:
      name: rc-app #label of rc, services will  Not use this.
spec:
    replicas: 5
    selector:
        type: app           #selector value and pods label needed to match,  even if selector is given
                            # pod label is must. If selector attribute  is not provided
                            # then pods label is taken as default selector for rc.

    template:        # pod details in template
        metadata: 
          name: firstpod
          labels:
            type: app # Services use this label for identification/ Mapping
        spec: 
          containers:
            - name: firstcontainer
              image: coolgourav147/nginx-custom
              env: 
                - name: owner
                  value: swapnil
    
>kubectl apply -f <rc.yaml>                     ---> repeatative usage for declaratibe object config.
>kubectl get rc  -o-wide / --show-labels

>kubectl delete -f <rc.yaml> / --cascade=false  ---> flag to keep recources in rc after deletion of rc.
>kubectl delete rc <rcname>  / --cascade=false

#manual scalling__
>kubectl scale rc --replicas=<n> <rc-name>   ---> imperative command way
>kubectl edit rc <rc-name>                   ---> imperative object configuration 
>edit in this yaml file                      ---> declarative object configuration 


#-------------------------------------------------ReplicaSet----------------------------------------------------

> we can consider, ReplicaSet as a further advancement of Replication Controller. It have advancement in
> selectors over the Replica set.
> IT USES  "SET BASED SELECTOR".

>read more: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/

apiVersion: apps/v1
kind: ReplicaSet

metadata:
    name: firstrs
    labels:
      name: rs-app
spec:
    replicas: 5
    selector:  
        matchLabels:  
          type: app   # Services use this label for identification/ Mapping   <label: key>

    template:
        metadata: 
          name: firstpod
          labels: 
            type: app # Services use this label for identification/ Mapping
            name: swapnil
        spec: 
          containers:
            - name: firstcontainer
              image: coolgourav147/nginx-custom
              env: 
                - name: owner
                  value: swapnil

#--------------rs_2------------------------

apiVersion: apps/v1
kind: ReplicaSet

metadata:
    name: firstrs
    labels:
      appname: app-rs
spec:
    replicas: 5
    selector:  
      matchExpressions:
        key: type 
        operator: In  # In/ NotIn
        values:
          - app 
          - app2
          - app3

    template:
      metadata: 
        name: pod1
          labels: 
            type: app       # notice the Key and Values in MatchExpression.
      spec:                # key is app and values can any from values.
        containers:
            name: firstcontainer
            image: coolgourav147/nginx-custom
               



apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: firstrs
  labels: 
    name: firstrs
spec:
  replicas: 5
  selector:
    matchExpressions:
      - key: app 
        operator: In 
        values: 
          - app1            # RS will consider the existing pods having labels as app, app2, frontend
          - app2

      - key: type
        operator: In 
        values:             # RS will not consider the existing pods having labels as backend and test
          - frontend   

      - key: type
        operator: NotIn 
        values:             # RS will not consider the existing pods having labels as backend and test
          - backend
          - test    

  template: 
    metadata: 
      name: pod 
      labels: 
        app: app 
        type: middleware  # this will be considered as it does  not fall in NotIn type
    spec: 
      containers:
        - name: container
          image: coolgourav147/nginx-custom      

> kubectl get rs   -o wide/ --show-labels
> kubectl apply -f <rs.yaml>
> kubectl delete -f <rs.yaml>
> kubectl delete rs <rs-name>    



#------------------------------------------------Deployment----------------------------------------------------

> we can consider,  Deployment as a further advancement of ReplicaSet. using which we can define different 
> startegies for deployment of our application, ensuring zero downtown. it allows us to rollback our deployment 
> in case of any error/bug with application.
> whenever we create a deployment, it will internally craetes a replica set.

> read : https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
> refer, dedicated deplyment code ....

>kubectl create deploy <name> --image=<image_name>

apiVersion: apps/v1
kind: Deployment

metadata:
  name: deploy
  labels: 
    name: deploy-app
spec:
  replicas: 5                 #BY DEFAULT ROLLING-UPDATE STRATEGY WILL BE APPLIED...
  selector:
    matchExpressions:  # matchLabels can also be used
      - key: app 
        operator: In 
        values: 
          - app
          - app2
          - app3

  template: 
    metadata: 
      name: pod 
      labels: 
        app: app 
    spec: 
      containers:
        - name: container
          image: coolgourav147/nginx-custom   



>strategy: rollingUpdate

apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy1
  labels: 
    name: deploy1
spec:
  replicas: 10
  minReadySeconds: 5  # this defines a time we give for our pod/app to ready after creation

  strategy:
    type: RollingUpdate 
    rollingUpdate:
      maxSurge: 0  #
      maxUnavailable: 2  # is the number of pod that will get replaced from old deployment when we change the
                         # the code, same number of new pods will be created

  selector:
    matchExpressions:
      - key: app 
        operator: In 
        values: 
          - app
          - app2

  template: 
    metadata: 
      name: pod 
      labels: 
        app: app 
    spec: 
      containers:
        - name: container
          image: coolgourav147/nginx-custom       

#---------------------------------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy1
  labels: 
    name: deploy1
spec:
  replicas: 10
  minReadySeconds: 5  # this defines a time we give for our pod/app to ready after creation
  strategy:
    rollingUpdate:
      maxSurge: 2       # is the new pod that will get created , without replacing the old pods
      maxUnavailable: 0 # is the number of pod that will get replaced from old deployment when we change the
    type: RollingUpdate                    #the code, same number of new pods will be created

  selector:
    matchExpressions:
      - key: app 
        operator: In 
        values: 
          - app
          - app2
          - app3

  template: 
    metadata: 
      name: pod 
      labels: 
        app: app 
    spec: 
      containers:
        - name: container
          image: coolgourav147/nginx-custom       


> if we explicity , do not gave the values of maxUnavailable or maxSurge, by default its value  will
> be 25% MaxUnavailable and 25% for MaxSurge


>--------------------------------------------------------------------------------
> kubectl apply -f <deploy.yaml>  ;  watch "kubectl get rs -o wide"
> kubectl rollout status deployment <deploy-name>          
> kubectl delete -f <.yml>
> kubectl delete deployment <deploy-name>
> kubectl rollout history deployment <deployment-name>
#------------------------------------------------------------------------------------------
> kubectl rollout history deployment <deployment-name>   --> will give all revision history, however this will
                                                           > not keep any explicit details of deployment.
> to keep deployment record, run a command as_
>  kubectl apply -f <deploy.yaml> --record, But this not add any custom massage.

> to add custom massage for deployment, add to yml file the following in metadata of deployemnt..
> under annotation , kubernetes.io/change-cause: <personalize text>  
> afte adding annotation, there is no need to use --record flag
#-------------------------------------------------------------------------------------------
> ROLLBACK_TO_SPECIFIC_REVISION_IN_HISTORY__
> kubectl rollout undo --to-revision=<specific-rev-number>  deployement  <deploy-name> 

> if specific revision number is not specified, by default it will choose most recent last revision. 
> .i.e. -->    kubectl rollout undo  deployement  <deploy-name>
#------------------------------------------------------------------------------------------

> by default, it will k8s , will keep the last "10" deployment record in memory, we can change this by using
> "revisionHistoryLimit: <n>"    , user spec of deployment.

>--------------------------------------------------------------------------------


>strategy:  Recreate

> in this startegy, all the pods from old deployemnt will simulateneously get replaced by pods from
> new deployment. this will cause the downtime, but new pods will get deployed faster.
> this startegy, generally not get used in production, but for testing , this will save the time.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy2
  labels: 
    name: deploy2
spec:
  replicas: 10
  strategy:
    type: Recreate 
  selector:
    matchLabels:
       app: app 

  template: 
    metadata: 
      name: pod 
      labels: 
        app: app 
    spec: 
      containers:
        - name: container
          image: coolgourav147/nginx-custom


>--------------------------------------------------------------------------------------------------------------


#--------------------------POD's resource REQUESTS and LIMITS------------------------------------------------

>putting resource limits in the pods specs itself.(resources for each container in pod)

apiVersion: v1
kind: Pod 

metadata: 
  name: pod2
  labels: 
     app: app
     type: front
spec: 
  containers:
    - name: firstcontainer
      image: coolgourav147/nginx-custom
      resources:
        requests:
          memory: 200Mi    #Applicable for this pod/container only.
          cpu: 100m
        limits: 
          memory: 300Mi
          cpu: 300m  

>requests :- if we define only requests, then pod will occupy the minimum resources as given in its request,
>            however , it may also occupy all the available resources as , limits are not defined.

> A limit can never be lower than the request. Kubernetes will error out if you attempt to do this. 
> If a container’s request is higher than a node’s capacity, Kubernetes will never schedule that container.

#------------------------------------LIMITS/ QUOTA in Namespace-------------------------------------

>Namespace is like seperate virtual space within kubernetes.
>kubernetes cluster may have multiple application deployed within same cluster. In this case, we can use seperate 
>namespaces for each application deployement, which introduced layer of isolation between them.

>Using namespace we can put limit on resource usage at namespace level, which ensure that each application 
>have enough resources, not all resources is consumed by one application only.

>kubectl api-resources
>kubectl create ns <ns-name>      ---> ns/namespace

> kubectl <option> <resource> *<name>  -n <ns-name>    ---> -n / --namespace
eg  kubectl get pod mypod -n testspace

to set custom Namespace as defualt
> kubectl config set-context --current --namespace=<ns-name>

To access service in another namespace 
> <*servicename>.<namespacename>.svc.cluster.local  -->curl it


>we can control resource allocation in custom namespace in two different ways_
1> ResourceQuota
2> LimitRange


> there are two types of quota/limits, 
>1) OBJECT BASED QUOTA, where we limit the number of Object that can be created inside the namespace.
>2) COMPUTE BASED QUOTA, where we limit the node resources to be used by k8s resource.

>----------------------------------------------------------------------------------------------

1) OBJECT BASED QUOTA (QUOTA FOR NAMESPACE)

>this will set the limits on the number of k8s objects that can be created in given namespace.

apiVersion: v1
kind: ResourceQuota
metadata: 
  name:  myquota   # Resource/ Object based Quota or limit
  namespace: my-namespace
  labels: 
    label2: key1
spec:
  hard:
    pods: 2    # same can be applied to RC, RS, DEPLOYMENTS, SERVICES, JOBS AND Others.........

> this will limit the namespace for creation of only two pods in it.
>Similarly, applicable for_ (Object Based Quota)
persistentvolumeclaims
services
secrets
configmaps
replicationcontrollers
deployments
replicasets
statefulsets
jobs
cronjobs

>----------------------------------------------------------------------------------------------

2) COMPUTE BASED QUOTA 

>This will set the combine limit on the quantity of the compute resource like RAM and CPU, a namespace can utilize.
>that can be used by thr k8s object in the given namespace.

apiVersion: v1                # this is the combine request/ limits for the pods or resoures in the namespace
kind: ResourceQuota           # i.e. SUM of all pods/resorces must be within request and limit constraint. 

metadata: 
  name:  myquota1 
  namespace: my-namespace          
spec:
  hard:
    requests.cpu: 0.5
    requests.memory: 200Mi   # Compute Based Quata or limit
    limits.cpu: 1
    limits.memory: 1Gi

                         # WE NEED TO DESCRIBE PODS REQUEST AND LIMIT PARAMETER IN POD SPECS/manifest....
                                # if we describe only limits in pods manifest file , then pod will request 
                                # the resources similar to limits. i.e. Request = Limits

apiVersion: v1
kind: ResourceQuota             #However, if we do not specify the limit in pod file and specify only request
metadata:                         # then apply will ERROR out, as it will cross the limit set by quota file for NS.
  name:  myquota1

spec:
  hard:
    pods: 10               #putting limits on number of resources and limit on compute resources..
    requests.cpu: 0.5                     # COMPUTE + OBJECT limit.
    requests.memory: 200Mi               
    limits.cpu: 1
    limits.memory: 1Gi                       


>kubectl apply -f <manifest.yaml> -n <namespaceName>

#---------------------------------------LIMIT RANGE--------------------------------------------

> to get rid of specifying the request and limit parameter in pod's spec file  
> we can use limit range.


apiVersion: v1 
kind:  LimitRange
metadata:
  name: testlimit
  namespace: my-namespace
spec:                             # type can be, Container, Pod, Image, ImageStream ,PersistentVolumeClaim (pvc)
  limits: 
    - default:
        cpu: 200m 
        memory: 300Mi
      type: Container   
>---------------------------------------------

apiVersion: v1
kind:  LimitRange
metadata:
  name: testlimit
  namespace: my-namespace
spec:
  limits:
    - default:
        cpu: 500m           # default limit for the pod/container inside ns
        memory: 300Mi
      defaultRequest:
        cpu: 100m           # default request for the pod/container inside ns
        memory: 150Mi

      min: 
        cpu: 80m         # must be smaller than  default request, this is the minimum object can have
        memory: 100Mi
      max:   
        cpu: 600m        # must be larger than default Limit, this is the maximum object can have
        memory: 500Mi 

      type: Container

>-----------------------------------------------------------
> we can also set the POD's/Containers's max Limit and Request in term of Ratio, as following

apiVersion: v1 
kind:  LimitRange
metadata: 
  name: testlimit
  namespace: my-namespace
spec:
  limits: 
    - maxLimitRequestRatio:   # limit:request   i.e    limit/request = 2
        memory: 2
      type: Container

> this specifies that pod's , limit/request must not exeed 2. 
> i.e. max request of pod must be less than or equivalent half of the limit.




#---------------------------------CONFIG MAP---------------------------------------------------------


a)-----------------------------------------------------

single property file: application.properties
#databse_details
database_ip="8.8.8.8"
database_password="12345"
database_username="swapnil"

#super_admin_details
username="swapnil"
password="12345"

>kubectl create configmap my-cm1 --from-literal=db_ip="8.8.8.8"   ---> from literal on cli
>kubectl create cm <my-cm3> --from-file=application.properties    ---> from file (imperative)
-----------------------------------------------------------------------

b)---------------------------------------------------------------------
multiple properties file from folder : properties
create needed number of properties file's inside folder: 
test1.properties
test2.properties
test3.properties

>kubectl create configmap <my-cm1> --from-file=properties/       ---> from folder
--------------------------------------------------------------------

c)-----------------------config map from environmental file-----------
file: env.sh

var1=val1
var2=val2
var3=val3

>kubectl create cm <my-cm5> --from-env-file=env.sh

-----------------------------------------------------------------------------

>creating config map via manifest file.

apiVersion: v1 
kind: ConfigMap

data:
  key1: value1
  key2: value2
  key3: value3
metadata: 
  name: configmap1
 


apiVersion: v1 
kind: ConfigMap

data:
  key1: |
    this is 
    key1 data
  key2: |
    this is 
    key2 data  in multiple
    lines

metadata: 
  name: configmap2

>kubectl apply -f <configmap.yaml>
>kubectl get cm
>kubectl describe cm <cm_name>

#----------------injecting config map into pod---------------------
                  
> accessing single key value pair in pod... 

apiVersion: v1
kind: Pod 

metadata: 
  name: configpod2
  labels:                                             #configMapKeyRef
     name: swapnil
spec: 
  containers:
    - name: container2
      image: coolgourav147/nginx-custom

      env:
       - name: variable-cm1
         valueFrom:
           configMapKeyRef:   # value of key1 from cm configmap1 will be available in pod by the name variable-cm1
             key: key1             
             name: configmap1   # name of configmap       
                       
       - name: variable-cm2
         valueFrom:
           configMapKeyRef: 
             key: key2          
             name: configmap2

       - name: variable-cm3
         valueFrom:
           configMapKeyRef:   # all the VALUES of test3.properties file will be available under 
             key: test3.properties   # variable-cm3 File inside the configmap       
             name: my-cm4 



apiVersion: v1             # accessing the entire config file.. configMapRef
kind: Pod 

metadata: 
  name: configpod4l
spec:                                       #configMapRef
  containers:
    - name: container4
      image: nginx

      envFrom:
       - configMapRef:         
          name: configmap1         
                             # getting all the values of configmap into pod...


>------------------------------------------
                   
                   # getting config as a file in to pod...
apiVersion: v1
kind: Pod 
metadata: 
  name: configpod5
spec: 
  containers:
    - name: container5
      image: nginx

      volumeMounts:
        - name: test
          mountPath: "/config"  
          readOnly: true

  volumes:
    - name: test
      configMap: 
        name: configmap1  # this will inject all the variable into pod as a individual file                                  
                         
                          # we can add following feild to get only selected vars as a file
        items:
          - key: var1
            path: myvar1                   



#-------------------------------------------SECRETS----------------------------- (uses Base64 encryption)


>kubectl create secret generic <my-sec1> --from-literal=name="swapnil"  --> direct from literals

>generic is encryption method, other are Docker registry and tls

single property file: application.properties
#databse_details
database_ip="8.8.8.8"                                    
database_password="12345"                                
database_username="swapnil"

>kubectl create secret generic <my-filesec> --from-file=application.properties  --> from file
>kubectl create secret generic <my-filesec> --from-file=properties/             --> from folder

from env file: env.sh
var1=val1
var2=val2
var3=val3

>kubectl create secret generic <my-envsec> --from-env-file=env.sh


apiVersion: v1
kind: Secret
                              
data:
  name: c3dhcG5pbA==               # first encrypt the data and then pass the value to manifest file.
  password: c3dhcG5pbA==

metadata:
  name: my-secrete1
                          #base64 value of swapnil i.e. we need to store encrypted values in yaml file.
                          #to get base 64 value
                          > echo -n <secrete> | base64

>kubectl create -f sec1.yaml


#-----injecting secret into pod------

#a) Injecting as Env variable in to pod.......

apiVersion: v1
kind: Pod 
metadata: 
  name: secret1
spec: 
  containers:
    - name: container5
      image: nginx

      env: 
       - name: myvar1
         valueFrom:
          secretKeyRef:                    #injecting single variable
              key: name                    # from above file
              name: my-secrete1 
       - name: myvar2
         valueFrom:
          secretKeyRef:                       #injecting single variable
              key: password                   # from above file
              name: my-secrete1 


apiVersion: v1
kind: Pod 
metadata: 
  name: secret1
spec: 
  containers:
    - name: container5          
      image: nginx

      envFrom:                # injecting all varible from file
       - secretRef: 
          name: my-filesec



 # injecting all varible as a file in to pod..

apiVersion: v1
kind: Pod 
metadata: 
  name: secret1
spec: 
  containers:
    - name: container5          
      image: nginx

      volumeMounts:
        - name: testvol
          mountPath: "/secrets"  
          readOnly: true

  volumes:                      # match the name
    - name: testvol               
      secret: 
        secretName: my-filesec

            

#--------------------------------------Taint and Toleration-----------------------------------------------   

> refer, https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/

Node affinity is a property of Pods that attracts them to a set of nodes,
(either as a preference or a hard requirement). Taints are the opposite,  they allow a node to Repel a set of pods.

>The default value for operator is Equal.

A toleration "matches" a taint if the keys are the same and the effects are the same, and:
>1.the operator is Exists (in which case no 'value' should be specified), or
>2.the operator is Equal and the values are equal.

There are two special cases:
>1.An empty key with operator Exists matches all keys, values and effects which means this will tolerate everything.
>2.An empty effect matches all effects with key key

>There can be multiple taints on the node with same or different keys, so in order for pod to get schedule on that 
>node, we need to add all the tolarations with all the keys, as per taints on the node.

>if there are multiple taints on node, pod need to tolerate all the taints , else it will not able to tolerate the
>taints on the node. 


> kubectl taint node <node-name> <key>=<taint>:<effect>    --> NoSchedule /PreferNoSchedule / NoExecute

Taint is putting the conditions on node for k8s resources to get scheduled on it.
if we taint the node , then only those pod, which can tolerate the taint of node will get schedule on
that node, else they will get schedule on another node .i.e NOSCHEDULE on tainted node.

> eg,  kubectl taint node nodename mysize=large:NoSchedule        / PreferNoSchedule / NoExecute

we can assign taint to node using above commands. 
above, we have tainted the node ,that these node is of large size,
and only those pod  which tolerate/will get schedule on that node who have 
condition/toleration attached to it. (mysize=large). .i.e. only if pod needs a large node size to get schedule.
for make pod tolerate the taint, we need to add toleration to pod. as below_


apiVersion: v1                       
kind: Pod                                             # Taint    :- Node
metadata:                                             # Tolerate :- Pod
  name: secret1         
spec: 
  containers:
    - name: container1         
      image: nginx
      imagePullPolicy: Never

  tolarations:                   # mysize=large:NoSchedule
    - effect: "NoSchedule"       # it will tolerate the NoSchedule taint on node.
      key: "mysize"                        # attaching tolerations so that it will tolerate the tainted node.
      operator: "equal"                    # it may also get schedule on non tainted available nodes.
      value: "large"

                              > operator : equal or exists
                                               
>in NoSchedule taint, if any pod is already i.e before assigning the taint is running on node
>then after assignment of taint, pod will remain on that node only.
>------------------------------------------------------------------------------------

> kubectl taint node <noddename> mysize=large:PreferNoSchedule

if we taint the node using above option , then the pod which will tolerate the taint of node will preferably
schedule on that node.
Howevewer other pods can also get schedule on it, even though it is not preferred node choice.
i.e. it does not gaurtees that other pods will not get schedule on it.


> kubectl taint node <noddename> mysize=large:NoExecute     
if any pod is already previously (before tainting) running on the tainted node , 
then that pod will also get deleted.

#to delete taint on node
>kubectl taint node <nodename> <taint>-          --> this (-) will delete the taint.
>kubectl taint node worker01 mysize-
>---------------------------------------------------------------------------

> Different effect of the tolerations...............

tolarations:
    - effect: NoSchedule 
      key: mysize             --> if operator is not given, by default it will consider the                            
      value: large                                "equal" operator


tolarations:
    - effect: ""         --> no matter what taint is, if effect is not specified, pod will tolerate all
      key: "mysize"                           i.e. all 3 effects
      operator: "equal"                             
      value: "large"


tolarations:
    - effect: NoSchedule    --> this will check taint "mysize" is available or not". No matter what the "value" is.
      key: mysize                    .i.e it do not tell to schedule or not to schedule on node.           
      operator: Exists        > pod can get schedule on any available node, including tainted one.                


tolarations:
    - effect: NoSchedule        --> it will tolerate everything
      key: ""                          
      operator: "Exists"                  


tolarations:
    - effect: ""               --> it will tolerate all
      key: ""                          
      operator: "Exists"                             
      value: ""



tolarations:
    - effect: "NoExecute"     # noExecute     
      key: "mysize"                          
      operator: "equal"   --> if the pod is available or assigned on the node, as soon as taint is assigned to Node,                
      value: "large"                 pod will tolerate the taint for 60 seconds, after that it will get Terminated
      tolerationSeconds: 60             > only iff no execute is effect.



#----------------------------------Scheduling (NODE SCHEDULING)----------------------------------------- 

>there are various ways to schedule a pod on node or select a Node to schedule/execute the pod,
>these are_
      1.Node Name and Node Selector
      2.Node Affinity and Node-Anti-Affinity
      3.Pod Affinity and Pod-Anti-Affinity
      4.Taints and Tolerations

>refer, https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/


>NODE SELECTOR_

>nodeSelector is a feild in pod spec, which defines the node on which we wants pod to get 
>schedule using lables attached to the node.

>it is the simplest form of recommandation for node selection constraint.

>it uses labels to select matching node on to which pod can be scheduled.  if the defined label does not exist
>pod will go in to pending state forever.

Assigning label to node_
> kubectl label node <nodename> <label>=<value>
> kubectl label node worker01 size=large

deleting the Node label
> kubectl label node <nodename> <label>-
> kubectl label node worker01 size-

apiVersion: v1
kind: Pod                                                    
metadata: 
  name: secret1
spec: 
  containers:
    - name: container5          
      image: nginx
      imagePullPolicy: Never

  nodeSelector:
      size: "large"    # all the pode with this selector will get schedule on the node with matching labels.

>make sure that node with specified label exist,  if label is not attached to node and we in pod specification
>gave the node selector then pod will remain in pending* state , as it will fails to find the node with label.  

#-------------------------------------------------------

>Node Name_

apiVersion: v1
kind: Pod
metadata:
  name: myapp
  labels:
    name: myapp
spec:
  containers:
  - name: myapp
    image: nginx
    ports:
      - containerPort: 80 

  nodeName: <node_name>    #name of node, NOT LABEL


>nodeName is a feild in pod spec, which defines the node name on which we wants pod to get schedule.
>it is simplest way to schedule the pod on perticular node using node selection constraiunts, but due
>to its limitations it is typically not used.
>we can also assign the pod to master node using this method.

>if the named node does not exist in cluster, the pod will not run and in some cases may be automatically deleted.
>like in eks cluster, we do not have choice for selecting node names.

>the the named node does not have a resource to accommodate the pod , the pod will fail.

>node name in cloud enevironments like eks or gke are not always predictable or stable.

to get names of the nodes in our k8s cluster.
>kubectl get nodes -o wide



#--------------------------------------------------AFFINITY---------------------------------------------------

>There are two types of Affinity, 
>NODE AFFINITY and POD AFFINITY, which is again divided in Affinity and AntiAffinity. (read on link below)

>refer, https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity


>NODE AFFINITY..............1>

# Assigning label to node_
> kubectl label node <nodename> size=large

apiVersion: v1
kind: Pod                                                    
metadata:                               >NODE AFFINITY
  name: secret1
spec: 
  containers:
    - name: container5          
      image: nginx
      imagePullPolicy: Never           
 
  affinity:
    nodeAffinity:                                               # preffered but not required
      preferredDuringSchedulingIgnoredDuringExecution:          # is is called SOFT SCHEDULING
        - preference:
            matchExpressions :
              - key: size
                operator: In           > operator can be, In/NotIn/Exist/NotExist etc...
                values:                        >label size=large
                  - large             #array, multiple values can be given
          weight : 10

> weight :- Weight is used to define the prefference of node selection, when we give multiple Match Expressions and 
>           and there are multiple nodes that satisfies the affinity conditions. so K8S rates the nodes and based 
>           on its values assign weight to it.
>           its value varies between 1 to 100, based on weight it "preffered" the node to get schedule.
>           (only for preffered). 
>           K8S will schedule the pod on a node with Highest total weight score. 
>           i.e weight 80 is more desirable than weight 40.

>--------------------------------------------2>

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: secret1
spec:                                          #NODE AFFINITY
  containers:
    - name: container5          
      image: nginx 
      imagePullPolicy: Never
                                       # Required, not just preffered, it is HARD scheduling
  affinity:                                    
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:     
        nodeSelectorTerms:    
          - matchExpressions :                         # size=large
              - key: size
                operator: In
                values: 
                  - large


>In above both cases, even if we remove the node label after pod is already scheduled on it
>pod will keep running on it. As both cases, have "IgnoredDuringExecution" policy attahed to it.

>requiredDuringSchedulingRequiredDuringExecution  --> do not exist in current k8s version 
>preferredDuringSchedulingRequiredDuringExecution --> do not exist in current k8s version   

>preferredDuringSchedulingIgnoredDuringExecution  --> exist in k8s (soft scheduling)
>requiredDuringSchedulingIgnoredDuringExecution   --> exist in k8s (hard scheduling)



#--------------------------------------------POD-AFFINITY---------------------------------------------------

# Pod Affinty and Pod antiAffinity_

>In pod Affinty and Pod antiAffinity , Node will select , How the perticular Pod will be placed Relative to other 
>pods based on the rules defined under Node affinity and AntiAffinity in pod spec manifest file.
>i.e. Node will select the pod , which it wants to execute/run on it.

>In pod Affinty and Pod antiAffinity , allows you to specify rules about How pods should be placed relative to
>other pods running on perticular node.

>Pod Affinity, can tell the scheduler to locate a new pod on the same node as 'Other Pod', 
>if the label selector on the new pod matches the label on current pod.
>i.e. it will check the label of existing pod, so that new pod will get schedule on the 
>same node as existing pod is. so that both pods will be on same node.

>Pod Anti Affinity, can Prevent the scheduler from locating a new pod on the same node as pod with the same label,
>if the label selector on the new pod matches the label on the current pod.

>in short, we use pod affinity and pod antiaffinity, to tell tell scheduler to co-locate the certain pod and
>not to co-locate the other pods, as per systems need.

>Pod Affinity_

apiVersion: v1
kind: Pod                                  # this is old/already existing pod
metadata:
  name: nginx-with-node-affinity
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx

  affinity: 
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: 
        nodeSelectorTerms: 
          - matchExpressions:
            - key: gpu 
              operator: Exists 
            #  value: amd  

>above node affinity, will schedule the pod on the node having label as 'gpu' , no matters whats its value
>may be. so out of multiple node having label 'gpu' , it will schedule the pod on any of those nodes.


apiVersion: v1
kind: Pod                           # this is new pod, and it will colocate with existing pod.
metadata:
  name: nginx-with-pod-affinity
spec:
  containers:
  - name: nginx
    image: nginx

  affinity: 
    podAffinity: 
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector: 
          matchExpressions:  
            key: app
            operator: Exists        # this will match the expression with labels of the existing pod.
            value: 
              - nginx
        topologyKey: gpu            # it will look for the node with label 'gpu', so that pods will colocate. 
   


>This pod affinity rule deploys the pod on Node , if it has GPU as the KEY and if it is already running a
>Pod that has labels mentioned as a labelSelector. 
>SO THAT NEW POD AND OLD POD WILL BE COLOCATE ON THE SAME NODE. IT WILL FIRST SEARCH FOR OLD POD WITH 
>ITS LABELS AND THEN IT WILL ALSO CHECK ITS TOPOLOGYKEY.


>topologyKey is the key for the node label. any label can be topologykey. 



#-------------------------------------------POD-ANTI-Affinity----------------------------------------------

> Using affinity , we can tell k8s engine, where to schedule the pod, but using AntiAffinity we can place 
> the condition to tell where to Schedule and Where to not Schedule the pods.

>Using AntiAffinity rule, we basically tell the k8s engine , where to not schedule the pod. so k8s engine will
>not schedule the pod as per labels/rules defined in antiaffinity rules.


>Pod AntiAffinity_

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    
>this pod will get schedule on any node, as we have not defined any rules for this pod to get schedule.

apiVersion: v1
kind: Pod                           # this is new pod, and it will not-colocate with existing pod.
metadata:
  name: nginx-with-pod-anti-affinity
spec:
  containers:
  - name: nginx
    image: nginx

  affinity: 
    podAntiAffinity: 
      prefferedDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm: 
          labelSelector:
            matchExpressions:  
            -  key: app
               operator: In        # this will match the expression with labels of the existing pod.
               value: 
                - nginx
          topologyKey: any-common-lable #kubernetes.io/hostname      
                                               
>                                               #we use common label, which every pod will have, becouse,  
>                                               #we do know the node on which our first pod will get deployed.


>Above Anti Affinity, rule states that , if the existing pod is already deployed on the node, then the pod with
>Anti Affinity rules must not get schedule on that node.

>This is achieved using Labels, in anti Affinity rules, we are defining the matchExpression rules,
>which searches for the the node, for the pod with matching label, if it found that pod is on that node,
>then the new pod will nt get scheduled on it.


>WE CAN USE BOTH AFFINITY AND ANTI-AFFINITY RULES TOGETHER, SO THAT DIFFERENT CONDITIONS CAN BE SATISFIED.
1.In general, we use AntiAffinity rules, when we do Not wants two pods to be co-located. So Anti Affinity
  will Repel Similar pods.
2.In general, we use Affinity rules, when we do wants two pods to be co-located. So Anti Affinity
  will Attracts Similar pods.

>-------end----




#-----------------------------------------k8s volumes-------------------------------------------------------- 

> Three types of volumes are :-> EmptyDir, HostPath, PersistantDisk.

>1)-------------emptyDir-----------------------

>if we delete the CONTAINER data will be preserved , and when new container is created by pod, all
>the data will be present in new container.

>i.e data volumes is created inside the POD, and will be available to Container inside that pod

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: vol1
spec: 
  containers:                     
    - name: container5          
      image: nginx
      imagePullPolicy: IfNotPresent

      volumeMounts:
        - mountPath: /data
          name: test-volume

  volumes:
    - name: test-volume  
      emptyDir: {}   

>data will remain inside the pod and will be available to the container inside that pod.
>this data will not be available to other containers on other pod or other pod on other machines.

>emptyDir is a temporary storage and it have a lifecycle similar to pod. i.e. once pod is deleted
>volume and data will also be lost.

>it is initially empty, emptydir volumes are stored on whatever medium is backing the node, that is
>it might be disk or n/w storage or RAM (emptyDir.medium feild to Memory)

>all containers in the pod can read and write to this volume by mounting the volume at some path in their
>file system.

>when pod is removed from node, data will be lost for ever, it is mainly used to store cache or temporary
>data to be processed.



>2)-------------HostPath-------------------------------

>if we delete the POD data will be preserved in directory in sync on Host Machine, 
>and when new POD is created it will have access to the data stored in dirctory on host.
>the data will be present in new POD inside the container.

>i.e data volumes is created inside the Host Machine, and will be available to POD's on that machine.

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: vol2
spec: 
  containers:                
    - name: container5          
      image: nginx
      imagePullPolicy: IfNotPresent

      volumeMounts:
        - mountPath: /data
          name: test-volume

  volumes:
    - name: test-volume  
      hostPath:
        path: /tmp/data   # at this path on Host machine data will be synchronised with containers data
                              # first, >minikube ssh , there /tmp folder path will be available.

>if we delete the pod and if pod get scheduled on another machine, then that pod will not have access to data
>available on previous machine.

>this type of volume mounts a file or directory created on node where pod is running.

>hostpath directory referes to directory created on node where pod is running.

>use it with caution becouse when pod are schedule on multiple nodes , each node gets own hostPath 
>storage volume. these may not be in sync with each other and different pods might be using a different data.

>when node become unstable, the pod might fails to access the hostpath directory and eventually gets terminated.
 


>check, PV AND PVC


>3)-------------EKS ON AWS-------------------------------- > Persistance data.                             

>In above two cases , if pod is get schedule on any machine , other than its original machine in cluster data will 
>no longer be available to the pod.
>In this all we need a volume which is available to the all the machines in the cluster, 
>Here AWS EKS comes to rescue.

>In this approch , we create a managed k8s cluster on aws eks, and create a ebs volume, which will remain
>available to the all nodes.  (managed cluster .i.e. AWS ITSELF WILL MANAGED MASTER NODE/CONTROL PLANE)

>If pod get schedule on a perticular node, then EBS volume will get attached to that node automatically and
>if we delete and create the pod , on another machines in cluster, then that EBS volume will automatically get
>attached to that machine. and this way pod will always have acces to the data.

>Create EKS cluster on K8s_
>1)install awscli
>2)with help of IAM user log in to aws account.
>3)start accessing aws services over awscli

#to create EKS cluster_
>eksctl create cluster --name <cluster-name> --node-type <type of ec2> --region <region> --node-zones <az>

>eksctl create cluster --name my-cluster --node-type t2.small --region ap-south-1 --node-zones ap-south-1a

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: vol2
spec: 
  containers:                
    - name: container5          
      image: nginx
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - mountPath: /data
          name: test-volume

  volumes:
    - name: test-volume
      awsElasticBlockStorage:                 # attach ebs volumes pod.
          volumeID: "vol-02c13c4470d26d461"
          fsType: ext4
      

>once our cluster get set up, it will also download the kube-config in to local machine. and we will 
>able to directaly use the kubectl commands on eks, No manual configuration is needed.

>----------------------------------------------------------------------------------------------------------


#--------------------POD LIVENESS ,READINESS PROBE and STARTUP PROBE-----------------------------

> refer, https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/

>--------------------------------Liveness Probe---------------------------------

>The kubelet uses liveness probes to know when to restart a container.
>The kubelet uses readiness probes to know when a container is ready to start accepting traffic.
>A Pod is considered ready when all of its containers are ready, i.e. when it passes the liveness probe.

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: live1
spec: 
  containers:                
    - name: cont1          
      image: nginx
      restartPolicy: Always        >Always(default), OnFailure, Never
      args: 
       - /bin/bash
       - -c
       - touch /tmp/test     # here , args will create a file test , this file will be checked by
      #  - sleep 10         # liveness probe at after 5 sec of container is ready and at regular interval of 3 sec.
      #  - rm -f /tmp/test  # as long this file is available for the probe to check, pod is consider as healthy.
      #  - sleep 20         # if we delete the file, then probe will no longer have acces to file and at next check, 
                            # probe will mark pod as unhealthy.
      livenessProbe:
        exec:                  
          command:
            - cat 
            - tmp/test
        initialDelaySeconds: 5
        timeoutSeconds: 3
        successThreshold: 2
        failureThreshold: 3

>If the command succeeds, it returns 0, and the kubelet considers the container to be alive and healthy. 
>If the command returns a non-zero value, the kubelet kills the container and restarts it.    
>Once, liveness probes have failed, and the failed containers will be killed and recreated.    

> apart from "command" in liveness probe, we can also use "http" request and "TCP" socket for the probe.        
> read on, Startup probe..........

>If the liveness probe fails, the kubelet kills the container, and the container is subjected to its restart policy.
>If a container does not provide a liveness probe, the default state is Success. 


>--------------------------------Readiness Probe-------------------------------

>even through pod is created and live, it do not means that it is ready to serve the requests. App may take 
>time to be ready to serve the traffic. we do readiness probe to check the same.

>Note: Readiness probes runs on the container during its whole lifecycle.
>Caution: Liveness probes do not wait for readiness probes to succeed. 
>If you want to wait before executing a liveness probe you should use initialDelaySeconds or a startupProbe.

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: live1
spec: 
  containers:                
    - name: cont1          
      image: nginx
      args: 
       - /bin/bash
       - -c
       - sleep 10
       - touch /tmp/test
                                  #liveness probe not necessary for readiness check.
      livenessProbe:
        exec:                  
          command:
            - cat 
            - tmp/test
        initialDelaySeconds: 10    # will check for liveness after 10 sec of pod creation, every 3 seconds.
        timeoutSeconds: 3

      readinessProbe:
        exec:
          command:
            - cat 
            - /tmp/test
        initialDelaySeconds: 20     # will check for readiness after 20 sec of pod creation every 5 seconds.
        timeoutSeconds: 5 
        successThreshold: 2
        failureThreshold: 3

>If the readiness probe fails, the endpoints controller removes the Pod's IP 
>address from the endpoints of all Services that match the Pod.

>The default state of readiness before the initial delay is Failure . 
>If a container does not provide a readiness probe, the default state is Success 


>-----------------------------------STARTUP-PROBE---------------------------------------------------------        


Protect slow starting containers with startup probes
Sometimes, you have to deal with legacy applications that might require an additional startup time on 
their first initialization. 

In such cases, it can be tricky to set up liveness probe parameters without compromising the 
fast response to deadlocks that motivated such a probe. 

The trick is to set up a startup probe with the same command, HTTP or TCP check, 
with a failureThreshold * periodSeconds long enough to cover the worse case startup time.

apiVersion: v1
kind: Pod
metadata:
  name: goproxy               #example just for ref, not to evaluate, may be incorrect.
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: registry.k8s.io/goproxy:0.1
    ports:
    - name: liveness-port
      containerPort: 8080
      hostPort: 8080

    livenessProbe:
      httpGet:
        path: /healthz
        port: liveness-port
      failureThreshold: 1
      periodSeconds: 10

    startupProbe:
      httpGet:
        path: /healthz
        port: liveness-port
      failureThreshold: 30
      periodSeconds: 10


Thanks to the startup probe, the application will have a maximum of 5 minutes (30 * 10 = 300s) to finish its
startup. Once the startup probe has succeeded once, the liveness probe takes over to provide a
fast response to container deadlocks. If the startup probe never succeeds,
the container is killed after 300s and subject to the pod's restartPolicy



>successThreshold  :- 
Minimum consecutive successes for the probe to be considered successful after having failed. 
Defaults to 1. Must be 1 for liveness and startup Probes. Minimum value is 1.


>failureThreshold  :- 
When a probe fails, Kubernetes will try failureThreshold times before giving up. 
Giving up in case of liveness probe means restarting the container. 
In case of readiness probe the Pod will be marked Unready. 
Defaults to 3. Minimum value is 1.

>initialDelaySeconds :- 
Number of seconds after the container has started before startup, 
liveness or readiness probes are initiated. Defaults to 0 seconds. Minimum value is 0.

>periodSeconds       :- 
 How often (in seconds) to perform the probe. Default to 10 seconds. 
Minimum value is 1.

>timeoutSeconds      :-   
Number of seconds after which the probe times out. 
Defaults to 1 second. Minimum value is 1.

>------------------------------------------------KUBECTL COPY----------------------------------------------------

> Copy files/dir to and From the Pod/Containers

>kubectl cp --help

Examples:
# !!!Important Note!!!
# Requires that the 'tar' binary is present in your container
# image.  If 'tar' is not present, 'kubectl cp' will fail.

Copy /tmp/foo_dir local directory to /tmp/bar_dir in a remote pod in the default namespace
>kubectl cp /tmp/foo_dir <some-pod>:/tmp/bar_dir

Copy /tmp/foo local file to /tmp/bar in a remote pod in a specific container
>kubectl cp /tmp/foo <some-pod>:/tmp/bar -c <specific-container>

Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace <some-namespace>
>kubectl cp /tmp/foo  <some-namespace>/<some-pod>:/tmp/bar

Copy /tmp/foo from a remote pod to /tmp/bar locally
>kubectl cp <some-namespace>/<some-pod>:/tmp/foo /tmp/bar

Options:
-c, --container='': Container name. If omitted, the first container in the pod will be chosen

Usage:
>kubectl cp <file-spec-src> <file-spec-dest> [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).



>------------------------------------------------DaemonSet-----------------------------------------------------

# DaemonSet_

>A DaemonSet is similar to the Deployments, it ensures that all (or some) Nodes run a copy of a Pod. 
>As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, 
>those Pods are garbage collected. 
>Deleting a DaemonSet will clean up the Pods it created.

>Both replica set and deployments are used for running a specificnumber of pods on k8s cluster.
>but certain cases exist , where we want to run a pod on each and every node, that exist in a cluster,
>and such node needs to run a exactly one instance only.

>In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon. 
>A more complex setup might use multiple DaemonSets for a single type of daemon, but with different
>flags and/or different memory and cpu requests for different hardware types


Some typical uses of a DaemonSet are:
>running a cluster storage daemon, such as Glustered, Ceph on every node.
>running a logs collection daemon, such as Filebeat, Fluented on every node.
>running a node monitoring daemon, such as Nagios, Elk, Prometheous on every node.


apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch

  template:
    metadata:
      labels:
        name: fluentd-elasticsearch  # same as containers
    spec:
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      volumes:
      - name: varlog
        hostPath:
          path: /var/log

>-------------------------------------------------------------------------------------------------------------


>read more_

>a)Containers_
1. Sidecar Container 
2. Ambassador Container 
3. Adapter Containers 
4. Init Containers 
5. Ephemeral Containers 
6. Static Containers

>b)Volumes_
1. Persistant Volume 
2. Persistant Volume Claim
3. EmptyDir
4. Host Path 


>c)Services_
  1. NodePort      (expose service out of cluser)
  2. LoadBalancer  (expose service out of cluser)
  3. ClusterIP     (expose service only within cluser)

  4. Ingress/Ingress_Resources/Ingress_Rule
  5. Ingress Controller (Nginx/Traefik)
  
  6. Port Forwarding
  7. External IP Service
  8. External Name Service
  9. Headless Service 

>d) K8S AutoScalling (HPA)
>e) Kubernetes DashBoard (skooner)
>f) Stateful Sets and Headless Service

>g) Service Account
>h) Roles/Cluster Role and Role Binding/Cluster Role Binding (CR&CRB)
>i) Role Based Access Control

>j) Pod/Node Affinity/AntiAffinity
>k) DaemonSet
>l) Istio/HAProxy/Nginx/Traefik
>m) Kubernetes Logs 
>n) Kubernetes Jobs
>o) custom resource and custom resource defination (CR&CRD)
>p> Docker Image Security using Anchore/Sync (DevSecOps)
>q>


>----------------------------------------------K8S Accessories-------------------------------------------------


#Kubernetes Package Managers and Plugins Managers

Kubernetes Krew plugin manager for kubectl :

>ref, https://krew.sigs.k8s.io/docs/

In devops environment, we may have to manage multiple k8s resources, across multiple
clusters.
we may also have different kubeconfig file for each environment or we hay have single kubeconfig
file multiple context and cluster.

>kubectl is a command line utitlity used to communicates with k8s API server to create , read, 
>update , delete (CRUD) workload within k8s.

>kubeconfig file is file used to configure access to the kubernetes cluster, when used in conjunction
>with kubectl command line tool.

>by default, kubectl looks for a file in  $HOME/.kube directory. we can specify other kubeconfig files by 
>setting the KUBECONFIG environment variable or by setting the --kubeconfig flag while running kubectl commands_
*kubectl --kubeconfig /pathto_/kube.config get pods

>Kubectl performs following task in k8s cluster_

Client Side Validation :
1. check yaml syntax
2. check for misconfigurations
3. check for non supported resource
4. fail fast mechanism to avoid loading the API server

Forms the HTTP request :
1. forms API calls
2. includes authentication tokens and certifications for user validations.
3. reads kubeconfig file
4. submits pod spec to the api server
5. Communicates with API server
                            
API server performs below task :
1. then api server perform user authentication and authorization.
2. verifies the APi
3. communicates with varies k8s components like ETCD, CONTROLLER, SCHEDULERS.


>use kubernetes lens tool for managing multiple clusters.

>to view the kube config details
kubectl config view

>to get context_
kubectl config get context

>to change context_
kubectl config use-context <context-name>

>to change/set namespace in kubeconfig file for upcoming commands
kubectl config set-context --current --namespace=<ns_name>



*krew_

1.krew is a plugin manager for kubectl command line tool
2.it discovers and installs kubectl plugins
3.there are 154 kubectl plugins currently distributed on krew
4.it works across all major platforms ,like Macos, linux and windows.

#Follow -> {google-->krew github -->documentation-->krew docs-->quickstart-->install&setup krew}

>install krew on Bash or ZSH shells
 Make sure that git is installed.

Run this command to download and install krew :

(
  set -x; cd "$(mktemp -d)" &&
  OS="$(uname | tr '[:upper:]' '[:lower:]')" &&
  ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &&
  KREW="krew-${OS}_${ARCH}" &&
  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz" &&
  tar zxvf "${KREW}.tar.gz" &&
  ./"${KREW}" install krew
)

>Add the $HOME/.krew/bin directory to your PATH environment variable.
To do this, update your .bashrc or .zshrc file and append the following line 
>export PATH="${KREW_ROOT:-$HOME/.krew}/bin:$PATH"
and restart your shell.

Run 
>kubectl krew 
to check the installation.

>Usage :-
  kubectl krew [command]

>Available Commands :-
  completion  generate the autocompletion script for the specified shell
  help        Help about any command
  index       Manage custom plugin indexes
  info        Show information about an available plugin
  install     Install kubectl plugins
  list        List installed kubectl plugins
  search      Discover kubectl plugins
  uninstall   Uninstall plugins
  update      Update the local copy of the plugin index
  upgrade     Upgrade installed plugins to newer versions
  version     Show krew version and diagnostics


>snap install kubectx --classic    ---> to install kubectx and kubens

# using ctx (context)              ---> k8s context utility

>kubectx                           ---> to list context
>kubectx -                         ---> to switch to the previous context
>kubectx <name_of_context>         ---> to change the context
>kubectx --help

# kubens (namespace)               ---> k8s namespace utility

>kubens                            ---> to list the available namespaces
>kubens <namepsace_name>           ---> to switch the context to namespace i.e to make ns default
>kubens -                          ---> t switch to the previous namespace in the current context
>kubens --help


#installing plugin with krew

>kubectl krew search <plugin_name>
eg1_ kubectl krew search score

>kubectl krew install <plugin_name>
eg1_ kubectl krew install score

>kubectl score <manifest.yaml>
eg1_ kubectl score pod1.yaml
eg1_ helm template bitnami/nginx | kubectl score -

-->score is a plugin for static code analysis for k8s object definations, it checks , if n/w policies are in place, 
   resource are set, affinity rules, probes, container security/context is set or not. 


eg2_ pod lense :- it will show pod related info like, namespace, service, node, secrets, cofigmaps etc.

>kubectl krew install pod-lens
>kubectl pod-lens <pod_name/deployment_name>


eg3_ prune-unused :- prune unused secrets/configmaps from current namespace
     checks against all resorces from mounted volume, env, and envFrom and imagePullSecrets.

>kubectl krew install prune-unused
>kubectl prune-unused secrets / configmaps --dry-run  

eg4_ neat :- neat removes clutter from k8s manifest file like creationtimestamps, resource ids, 
             status information to make them more readable.

>kubectl krew install neat
>kubectl get pod <pod_name> -o wide | kubectl neat

eg5_ tree :- tree explores the ownership relationships between kubernetes objects 
              through owners references on them.

>kubectl krew install tree
>kubectl tree <resource> <name>              


eg6_ tail :- tail streams a logs from all containers of all matched pods and can match pods by service,
             replicaset, deployment and others.
>kubectl krew install tail
>kubectl tail -p <name_of_pod>

>-l : --label , -p : --pod, -n : --ns ,-svc , -rc, -rs, -d, -sts            

eg7_ popeye :- scans live k8s cluster and reports potential issues with deployed resources and configurations.
               it detects misconfigurations and helps us to ensure that best prectices are in place, thus
               preventing future headaches.

>kubectl krew install popeye
>kubectl popeye                 
it will scans the cluster and give details report on it.


>----------------------------------------------------------------------------------------------------------

#Kubernetes Tool

DATREE : 

Datree is a CLI tool that supports Kubernetes owners in their roles, by preventing developers from making 
errors in Kubernetes configurations that can cause clusters to fail in production. 
Our CLI solution is open source, enabling it to be supported by the Kubernetes community.


>install datree on linux
curl https://get.datree.io | /bin/bash

>Run a policy check against a Kubernetes manifest
datree test <k8s_manifest.yaml>

>Enable/disable built-in rules
After signing up, you'll be automatically redirected to your Centralized policy where you can configure 
it according to your needs.




>----------------------------------------------------------------------------------------------------------