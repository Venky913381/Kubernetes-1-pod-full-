>
>--------------------------------------BASIC-K8S-CLI-INSTRUCTIONS-----------------------------------------------

>kubectl get <type>  
>kubectl get <type> <name>
>kubectl get <type> <name> -o wide/yaml/json  > foo.yaml/yoo.json

>kubectl logs <pod_name>  -f                       ---> to get logs of the k8s pod/container
>kubectl logs <container_name>                     ---> to get logs of the k8s pod
>kubectl logs <pod_name> -c <container_name>       ---> -f for continuos logs
>kubectl logs -f <pod_name> --all-containers

>kubectl exec <podname> -c <container_name> -it -- <command>   ---> to execute into running pod/container

>kubectl run <podname> --image=<image:version>     ---> Imperative command
>kubectl create -f <manifest.yaml>                 ---> Imperative object configuration
>kubectl edit <type> <name>                        ---> imperative object configuration

>kubectl apply -f <manifest.yaml>                     ---> declarative object configuration
>kubectl apply -f ./ or < relative path>              ---> to apply all yaml manifests in folder
>kubectl apply -f <manifest.yaml> --dry-run=client    ---> to test/dryrun the current configurations

>kubectl explain <k8s resource>              ---> to get basic manifest component details
>kubectl explain --recursive <type> | less   ---> to get all resource attributes for writting manifest
>kubectl describe <type> <name>              ---> to get more details on the k8s resource

>kubectl label <type> <name> label1=value1
>kubectl label --override <type> <name> label1=value2   ---> to override the existing label
>kubectl label <type> --all label1=value1               ---> to label all resource type in one go
>kubectl label <type> <name> label1-                    ---> (-) to delete the lable
>kubectl get <type> <name> --show-labels / -o wide

>kubectl delete <type> <name>
>kubectl delete -f <manifest.yaml>    ---> will delete all the resources declared in manifest file.
>kubectl delete -f ./path             ---> will delete all the resources declared in manifest folder.
>kubectl delete all --all             ---> will delete all k8s resources running

>kubectl get nodes -o wide    ---> to get details on the node participating in cluster
>kubectl cluster-info         ---> gives the cluster details
>kubectl api-resources        ---> gives the lost of resources, short names, namespace support,version, kind etc.

>minikube --help           ---> for minikube commands.
>minikube start --force    ---> to start minikube on booted/restarted machine
>minikube status           ---> Gets the status of a local Kubernetes cluster
>minikube stop             ---> Stops a running local Kubernetes cluster
>minikube delete           ---> Deletes a local Kubernetes cluster
>minikube dashboard        ---> Access the Kubernetes dashboard running within the minikube cluster
>minikube pause            ---> pause Kubernetes
>minikube unpause          ---> unpause Kubernetes
>minikube ip               ---> to get cluster ip


#-------------------------------------------POD------------------------------------------------

>read : https://kubernetes.io/docs/concepts/workloads/pods/
>kubectl run <podname> --image=<image:version>     ---> Imperative command

apiVersion: v1
kind: Pod 

metadata: 
  name: firstpod
  labels: 
     env: prod
     name: swapnil
  namespace: test-space   
spec: 
  containers:
    - name: firstcontainer
      image: coolgourav147/nginx-custom
      env:                                  >  env variables are key:value pairs wriitten as array.
        - name: owner
          value: swapnil
          name: city 
          value: pune 
      args: ["sleep","3600"]  
  initContainers:
    - name: init1container
      image: coolgourav147/nginx-custom
      env: 
        - name: name
          value: swapnil
          name: city 
          value: nagpur
      args: ["sleep","15"] 

    - name: init2container
      image: coolgourav147/nginx-custom
      env: 
        - name: name
          value: swapnil
          name: city 
          value: nagpur
      args: ["sleep","15"]

>kubectl apply -f <filename.yaml>  / --dry-run=client
>kubectl get pod / -o wide / --show-labels / -w
>kubectl exec <podname> -c <containerName> -it -- <command>     --> executing command inside the container.
>                       --container <containerName> -it -- <bash/cmd>

#------------------------------------------service---------------------------------------------------

read more on services: https://kubernetes.io/docs/concepts/services-networking/service/

>service is k8s resource used to expose the pod/containers for it to accept the outside/internet traffic. 
>Using service we open the port on the pod.

apiVersion: v1
kind: Service

metadata:
      name: firstservice
      labels:
        servicelabel: label1
spec:
      type: NodePort
      selector:
          type: app
      port:
        - nodePort: 32000
          port: 9000
          targetPort: 80
      
 #--------------------------------POD AND SERVICE------------------

apiVersion: v1
kind: Pod
metadata: 
  name: pod1 
  labels: 
     type: app   #this label is use by selector of service
spec:
  containers:
    - image:  coolgourav147/nginx-custom
      name:  firstcontainer


apiVersion: v1                
kind: Service
metadata:
      name: service1
      labels:                                            ip:80---->9000---->32000
        servicelabel: label1
spec:
      type: NodePort
      port:
        - nodePort: 32000  > on container
          port: 9000       > on pod , machine port will mapped to this port on pod
          targetPort: 80   > on machine / browser / ip:port / this is endpoint
      selector:
          type: app  # this must be same as pods label, key-value pair

>-----------------------------------------------------------------------------

>1) http://ip:8080 ---->9000---->32000 --->app (selector)
>2) http://ip:8080 ---->9000---->my-service  --->app (selector)


  selector:
          type: app
  ports:                                               
  - nodePort: 8080
    port: 9000      
    targetPort: 32000  

    name: my-service _ read below
    protocol: tcp

> for using "name" container specs must mentioned the Port attributes with containerport and its name in container
> spec as below.....
ports:
- containerPort: 32000        
  name: my-service  



>kubectl expose pod <podname> --type=<NodePort/ClusterIP/lb> --port=<port> --target-port=<port> --name <svcName>
>kubectl apply -f <filename.yaml>  / --dry-run=client
>kubectl get service/svc    -o wide / --show-labels


>kubectl port-forward --address 0.0.0.0 svc/<serviceName> <OnHost>:<OnPod>
>kubectl port-forward --address 0.0.0.0 svc/<Service-Name> --namespace <NameSpace Name> <Machine/HostPort:PodPort>


#-----------------------------------------ReplicationController-----------------------------------------------

>read: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/

>USES ONLY EQUILITY BASED SELECTOR.

apiVersion: v1                      
kind: ReplicationController

metadata:
    name: firstrc
    labels:
      name: rc-app #label of rc, services will  Not use this.
spec:
    replicas: 5
    selector:
        type: app         #selector value and pods label needed to match,  even if selector is given
                                                 # pod label is must. If selector attribute  is not provided
                                                  # then pods label is taken as default selector for rc.
    template:        # pod details in template
        metadata: 
          name: firstpod
          labels:
            type: app # Services use this label for identification/ Mapping
        spec: 
          containers:
            - name: firstcontainer
              image: coolgourav147/nginx-custom
              env: 
                - name: owner
                  value: swapnil
    
>kubectl apply -f <rc.yaml>                --> repeatative usage for declaratibe object config.
>kubectl get rc  -o-wide / --show-labels

>kubectl delete -f <rc.yaml> / --cascade=false  ---> flag to keep recources in rc after deletion of rc.
>kubectl delete rc <rcname>  / --cascade=false

#manual scalling__
>kubectl scale rc --replicas=<n> <rc-name>   ---> imperative command way
>kubectl edit rc <rc-name>                   ---> imperative object configuration 
>edit in this yaml file                      ---> declarative object configuration 


#-------------------------------------------------ReplicaSet----------------------------------------------------

> we can consider, ReplicaSet as a further advancement of Replication Controller. It have advancement in
> selectors over the Replica set.
> USES EQUILITY BASED SELECTOR AS WELL AS SET BASED SELECTOR.

>read more: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/

apiVersion: apps/v1
kind: ReplicaSet

metadata:
    name: firstrs
    labels:
      name: rs-app
spec:
    replicas: 5
    selector:  
        matchLabels:  
          type: app   # Services use this label for identification/ Mapping   <label: key>

    template:
        metadata: 
          name: firstpod
          labels: 
            type: app # Services use this label for identification/ Mapping
            name: swapnil
        spec: 
          containers:
            - name: firstcontainer
              image: coolgourav147/nginx-custom
              env: 
                - name: owner
                  value: swapnil

#--------------rs_2------------------------

apiVersion: apps/v1
kind: ReplicaSet

metadata:
    name: firstrs
    labels:
      appname: app-rs
spec:
    replicas: 5
    selector:  
      matchExpressions:
        key: type 
        operator: In  # In/ NotIn
        values:
          - app 
          - app2
          - app3

    template:
      metadata: 
        name: pod1
          labels: 
            type: app       # notice the Key and Values in MatchExpression.
      spec:                # key is app and values can any from values.
        containers:
            name: firstcontainer
            image: coolgourav147/nginx-custom
               



apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: firstrs
  labels: 
    name: firstrs
spec:
  replicas: 5
  selector:
    matchExpressions:
      - key: app 
        operator: In 
        values: 
          - app1       # RS will consider the existing pods having labels as app, app2, frontend
          - app2

      - key: type
        operator: In 
        values:             # RS will not consider the existing pods having labels as backend and test
          - frontend   

      - key: type
        operator: NotIn 
        values:             # RS will not consider the existing pods having labels as backend and test
          - backend
          - test    

  template: 
    metadata: 
      name: pod 
      labels: 
        app: app 
        type: middleware  # this will be considered as it does  not fall in NotIn type
    spec: 
      containers:
        - name: container
          image: coolgourav147/nginx-custom      

> kubectl get rs   -o wide/ --show-labels
> kubectl apply -f <rs.yaml>
> kubectl delete -f <rs.yaml>
> kubectl delete rs <rs-name>    



#------------------------------------------------Deployment----------------------------------------------------

> we can consider,  Deployment as a further advancement of ReplicaSet. using which we can define different 
> startegies for deployment of our application, ensuring zero downtown. it allows us to rollback our deployment 
> in case of any error/bug with application.
> whenever we create a deployment, it will internally craetes a replica set.

> read : https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
> refer, dedicated deplyment code ....

apiVersion: apps/v1
kind: Deployment

metadata:
  name: deploy
  labels: 
    name: deploy-app
spec:
  replicas: 5                 #BY DEFAULT ROLLING-UPDATE STRATEGY WILL BE APPLIED...
  selector:
    matchExpressions:  # matchLabels can also be used
      - key: app 
        operator: In 
        values: 
          - app
          - app2
          - app3

  template: 
    metadata: 
      name: pod 
      labels: 
        app: app 
    spec: 
      containers:
        - name: container
          image: coolgourav147/nginx-custom   



apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy1
  labels: 
    name: deploy1
spec:
  replicas: 10
  minReadySeconds: 5  # this defines a time we give for our pod/app to ready after creation

  strategy:
    type: RollingUpdate 
    rollingUpdate:
      maxSurge: 0  #
      maxUnavailable: 2  # is the number of pod that will get replaced from old deployment when we change the
                         # the code, same number of new pods will be created

  selector:
    matchExpressions:
      - key: app 
        operator: In 
        values: 
          - app
          - app2

  template: 
    metadata: 
      name: pod 
      labels: 
        app: app 
    spec: 
      containers:
        - name: container
          image: coolgourav147/nginx-custom       

#---------------------------------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy1
  labels: 
    name: deploy1
spec:
  replicas: 10
  minReadySeconds: 5  # this defines a time we give for our pod/app to ready after creation
  strategy:
    rollingUpdate:
      maxSurge: 2  # is the new pod that will get created , without replacing the old pods
      maxUnavailable: 0 # is the number of pod that will get replaced from old deployment when we change the
    type: RollingUpdate                    #the code, same number of new pods will be created

  selector:
    matchExpressions:
      - key: app 
        operator: In 
        values: 
          - app
          - app2
          - app3

  template: 
    metadata: 
      name: pod 
      labels: 
        app: app 
    spec: 
      containers:
        - name: container
          image: coolgourav147/nginx-custom       


> if we explicity , do not gave the values of maxUnavailable or maxSurge, by default its value  will
> be 25% MaxUnavailable and 25% for MaxSurge

#---------------------------------------------------------------


apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy2
  labels: 
    name: deploy2
spec:
  replicas: 10
  strategy:
    type: Recreate 
  selector:
    matchLabels:
       app: app 
  template: 
    metadata: 
      name: pod 
      labels: 
        app: app 
    spec: 
      containers:
        - name: container
          image: coolgourav147/nginx-custom


>--------------------------------------------------------------------------------------------------------------

#--------------------------POD's resource REQUESTS and LIMITS------------------------------------------------

>putting resource limits in the pods specs itself.

apiVersion: v1
kind: Pod 

metadata: 
  name: pod2
  labels: 
     app: app
     type: front
spec: 
  containers:
    - name: firstcontainer
      image: coolgourav147/nginx-custom
      resources:
        requests:
          memory: 200Mi    #Applicable for this pod only.
          cpu: 100m
        limits: 
          memory: 300Mi
          cpu: 300m  

>requests :- if we define only requests, then pod will occupy the minimum resources as given in its request,
>            however , it may also occupy all the available resources as , limits are not defined.

> A limit can never be lower than the request. Kubernetes will error out if you attempt to do this. 
> If a container’s request is higher than a node’s capacity, Kubernetes will never schedule that container.

#------------------------------------LIMITS/ QUATA in Namespace-------------------------------------

>Namespace is like seperate virtual space within kubernetes.
>kubernetes cluster may have multiple application deployed within same cluster. In this case, we can use seperate 
>namespaces for each application deployement, which introduced layer of isolation between them.

>Using namespace we can put limit on resource usage at namespace level, which ensure that each application 
>have enough resources, not all resources is consumed by one application only.

>kubectl api-resources
>kubectl create ns <ns-name>      ---> ns/namespace

> kubectl <option> <resource> *<name>  -n <ns-name>    ---> -n / --namespace
eg  kubectl get pod mypod -n testspace

to set custon Namespace as defualt
> kubectl config set-context --current --namespace=<ns-name>

To access service in another namespace 
> <*servicename>.<*namespacename>.svc.cluster.local  -->curl it


> there are two types of quota/limits, 
>1) OBJECT BASED QUOTA, where we limit the number of Object that can be created inside the namespace.
>2) COMPUTE BASED QUOTA, where we limit the node resiources to be used by k8s resource.

>----------------------------------------------------------------------------------------------

1) OBJECT BASED QUOTA

>to set limits and quota in namespace in yml_

apiVersion: v1
kind: ResourceQuota
metadata: 
  name:  myquota   # Resource/ Object based Quata or limit
  namespace: my-namespace
  labels: 
    label2: key1
spec:
  hard:
    pods: 2    # same can be applied to RC, RS, DEPLOYMENTS, SERVICES, JOBS AND Others.........

> this will limit the namespace for creation of only two pods in it.

>----------------------------------------------------------------------------------------------

2) COMPUTE BASED QUOTA

apiVersion: v1                # this is the combine request/ limits of the pods or resoures in the namespace
kind: ResourceQuota           # i.e. SUM of all pods/resorces must be within request and limit constraint. 

metadata: 
  name:  myquota1 
  namespace: my-namespace          
spec:
  hard:
    requests.cpu: 0.5
    requests.memory: 200Mi   # Compute Based Quata or limit
    limits.cpu: 1
    limits.memory: 1Gi

                         # WE NEED TO DESCRIBE PODS REQUEST AND LIMIT PARAMETER IN POD SPECS/manifest....
                                # if we describe only limits in pods manifest file , then pod will request 
                                # the resources similar to limits. i.e. Request = Limits
apiVersion: v1
kind: ResourceQuota             #However, if we do not specify the limit in pod file and specify only request
metadata:                         # then apply will ERROR out, as it will cross the limit set by quata file for NS.
  name:  myquota1

spec:
  hard:
    pods: 10               #putting limits on number of resources and limit on compute resources..
    requests.cpu: 0.5                     # COMPUTE + OBJECT limit.
    requests.memory: 200Mi               
    limits.cpu: 1                       


#---------------------------------------LIMIT RANGE--------------------------------------------

> to get rid of specifying the request and limit parameter in pod file  
> we can use limit range.................


apiVersion: v1 
kind:  LimitRange
metadata:
  name: testlimit
  namespace: my-namespace
spec:                             # type can be, Container, POd, Image, ImageStream ,PersistentVolumeClaim (pvc)
  limits: 
    - default:
        cpu: 200m 
        memory: 300Mi
      type: Container   
>---------------------------------------------

apiVersion: v1
kind:  LimitRange
metadata:
  name: testlimit
  namespace: my-namespace
spec:
  limits:
    - default:
        cpu: 500m           # default limit for the pod/container inside ns
        memory: 300Mi
      defaultRequest:
        cpu: 100m           # default request for the pod/container inside ns
        memory: 150Mi

      min: 
        cpu: 80m         # must be smaller than  default request
        memory: 100Mi
      max:   
        cpu: 600m       # must be larger than default Limit
        memory: 500Mi 

      type: Container

>-----------------------------------------------------------
> we can also set the POD's/Containers's max Limit and Request in term of Ratio, as following

apiVersion: v1 
kind:  LimitRange
metadata: 
  name: testlimit
  namespace: my-namespace
spec:
  limits: 
    - maxLimitRequestRatio: 
        memory: 2
      type: Container

> this specifies that pod's , limit/request must not exeed 2. 
> i.e. max request of pod must be less than or equivalent half of the limit.




#---------------------------------CONFIG MAP---------------------------------------------------------


a)-----------------------------------------------------

single property file: application.properties
#databse_details
database_ip="8.8.8.8"
database_password="12345"
database_username="swapnil"

#super_admin_details
username="swapnil"
password="12345"

>kubectl create configmap my-cm1 --from-literal=db_ip="8.8.8.8"   ---> from literal o cli
>kubectl create cm <my-cm3> --from-file=application.properties    ---> from file (imperative)
-----------------------------------------------------------------------

b)---------------------------------------------------------------------
multiple properties file from folder : properties
create needed number of properties file's inside folder: 
test1.properties
test2.properties
test3.properties

>kubectl create configmap <my-cm1> --from-file=properties/       ---> from folder
--------------------------------------------------------------------

c)-----------------------config map from environmental file-----------
file: env.sh

var1=val1
var2=val2
var3=val3

>kubectl create cm <my-cm5> --from-env-file=env.sh

-----------------------------------------------------------------------------


apiVersion: v1 
kind: ConfigMap

data:
  key1: value1
  key2: value2
  key3: value3
metadata: 
  name: configmap1
 


apiVersion: v1 
kind: ConfigMap

data:
  key1: |
    this is 
    key1 data
  key2: |
    this is 
    key2 data  in multiple
    lines

metadata: 
  name: configmap2


#----------------injecting config map into pod---------------------
                  
                # accessing single key value pair in pod... 

apiVersion: v1
kind: Pod 

metadata: 
  name: configpod2
  labels:                                             #configMapKeyRef
     name: swapnil
spec: 
  containers:
    - name: container2
      image: coolgourav147/nginx-custom

      env:
       - name: variable-cm1
         valueFrom:
           configMapKeyRef:   # value of key1 from cm configmap1 will be available in pod by the name variable-cm1
             key: key1             
             name: configmap1   # mane of configmap       
                       
       - name: variable-cm2
         valueFrom:
           configMapKeyRef: 
             key: key2          
             name: configmap2

       - name: variable-cm3
         valueFrom:
           configMapKeyRef:   # all the VALUES of test3.properties file will be available under variable-cm3
             key: test3.properties   # file inside the configmap       
             name: my-cm4 



apiVersion: v1             # accessing the entire config file.. configMapRef
kind: Pod 

metadata: 
  name: configpod4l
spec:                                       #configMapRef
  containers:
    - name: container4
      image: nginx

      envFrom:
       - configMapRef:         
          name: configmap1         
                             # getting all the values of configmap into pod...


>------------------------------------------
                   
                   # getting config as a file in to pod...
apiVersion: v1
kind: Pod 
metadata: 
  name: configpod5
spec: 
  containers:
    - name: container5
      image: nginx

      volumeMounts:
        - name: test
          mountPath: "/config"  
          readOnly: true

  volumes:
    - name: test
      configMap: 
        name: configmap1  # this will inject all the variable into pod as a individual file                                  
                         
                          # we can add following feild to get only selected vars as a file
        items:
          - key: var1
            path: myvar1                   



#-------------------------------------------SECRETS----------------------------- (uses Base64 encryption)


>kubectl create secret generic <my-sec1> --from-literal=name=swapnil  --> direct from literals

> generic is encryption method, other are Docker registry and tls

single property file: application.properties
#databse_details
database_ip="8.8.8.8"                                    
database_password="12345"                                
database_username="swapnil"

>kubectl create secret generic <my-filesec> --from-file=application.properties  --> from file
>kubectl create secret generic <my-filesec> --from-file=properties/             --> from folder
from env file: env.sh

var1=val1
var2=val2
var3=val3

>kubectl create secret generic <my-envsec> --from-env-file=env.sh


apiVersion: v1
kind: Secret
                              
data:
  name: c3dhcG5pbA==               # first encrypt the data and then pass the value to manifest file.
  password: c3dhcG5pbA==

metadata:
  name: my-secrete1
                          #base64 value of swapnil i.e. we need to store encrypted values in yaml file.
                          #to get base 64 value
                          > echo -n <secrete> | base64

>kubectl create -f sec1.yaml


#-----injecting secret into pod------

#a) Injecting as Env variable in to pod.......

apiVersion: v1
kind: Pod 
metadata: 
  name: secret1
spec: 
  containers:
    - name: container5
      image: nginx

      env: 
       - name: myvar1
         valueFrom:
          secretKeyRef:        #injecting single variable
              key: name                    # from above file
              name: my-secrete1 
       - name: myvar2
         valueFrom:
          secretKeyRef:        #injecting single variable
              key: password                   # from above file
              name: my-secrete1 


apiVersion: v1
kind: Pod 
metadata: 
  name: secret1
spec: 
  containers:
    - name: container5          
      image: nginx
      envFrom:                # injecting all varible from file
       - secretRef: 
          name: my-filesec



 # injecting all varible as a file in to pod..

apiVersion: v1
kind: Pod 
metadata: 
  name: secret1
spec: 
  containers:
    - name: container5          
      image: nginx

      volumeMounts:
        - name: testvol
          mountPath: "/secrets"  
          readOnly: true

  volumes:                      # match the name
    - name: testvol               
      secret: 
        secretName: my-filesec

            

#--------------------------------------Taint and Toleration-----------------------------------------------   

> refer, https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/

> kubectl taint node <node-name> <key>=<taint>:<effect>    --> NoSchedule /PreferNoSchedule / NoExecute
> eg,  kubectl taint node nodename mysize=large:NoSchedule        / PreferNoSchedule / NoExecute

Taint is putting the conditions on node for k8s resources to get scheduled on it.
if we taint the node , then only those pod, which can tolerate the taint of node will get schedule on
that node, else they will get schedule on another node .i.e NOSCHEDULE on tainted node.

we can assign taint to node using above commands. 
above, we have tainted the node ,that these node is of large size,
and only those pod  which tolerate/will get schedule on that node who have 
condition/toleration attached to it. (mysize=large). .i.e. only if pod needs a large node size to get schedule.
for make pod tolerate the taint, we need to add toleration to pod. as below_


apiVersion: v1                       
kind: Pod                                             # Taint    :- Node
metadata:                                             # Tolerate :- Pod
  name: secret1         
spec: 
  containers:
    - name: container1         
      image: nginx
      imagePullPolicy: Never

  tolarations:                   # mysize=large:NoSchedule
    - effect: "NoSchedule"       # it will tolerate the NoSchedule taint on node.
      key: "mysize"                        # attaching tolerations so that it will tolerate the tainted node.
      operator: "equal"                    # it may also get schedule on non tainted available nodes.
      value: "large"

                              > operator : equal or exists
                                               
>in NoSchedule taint, if any pod is already i.e before assigning the taint is running on node
>then after assignment of taint, pod will remain on that node only.
>------------------------------------------------------------------------------------

> kubectl taint node <noddename> mysize=large:PreferNoSchedule

if we taint the node using above option , then the pod which will tolerate the taint of node will preferably
schedule on that node.
Howevewer other pods can also get schedule on it, even though it is not preferred node choice.
i.e. it does not gaurtees that other pods will not get schedule on it.


> kubectl taint node <noddename> mysize=large:NoExecute     
if any pod is already previously (before tainting) running on the tainted node , 
then that pod will also get deleted.

#to delete taint on node
>kubectl taint node <nodename> <taint>-          --> this (-) will delete the taint.
>kubectl taint node worker01 mysize-
>---------------------------------------------------------------------------

> Different effect of the tolerations...............

tolarations:
    - effect: NoSchedule 
      key: mysize             --> if operator is not given, by default it will consider the                            
      value: large                                "equal" operator


tolarations:
    - effect: ""         --> no matter what taint is, if effect is not specified, pod will tolerate all
      key: "mysize"                           i.e. all 3 effects
      operator: "equal"                             
      value: "large"


tolarations:
    - effect: NoSchedule    --> this will check taint "mysize" is available or not". No matter what the "value" is.
      key: mysize                    .i.e it do not tell to schedule or not to schedule on node.           
      operator: Exists        > pod can get schedule on any available node, including tainted one.                


tolarations:
    - effect: NoSchedule        --> it will tolerate everything
      key: ""                          
      operator: "Exists"                  


tolarations:
    - effect: ""               --> it will tolerate all
      key: ""                          
      operator: "Exists"                             
      value: ""



tolarations:
    - effect: "NoExecute"     # noExecute     
      key: "mysize"                          
      operator: "equal"   --> if the pod is available or assigned on the node, as soon as taint is assigned to Node,                
      value: "large"                 pod will tolerate the taint for 60 seconds, after that it will get Terminated
      tolerationSeconds: 60             > only iff no execute is effect.



#----------------------------------Scheduling (NODE SCHEDULING)----------------------------------------- 

>there are various ways to schedule a pod on node or selecta Node to schedule/ececute the pod,
>these are_
      1.Node Name and Node Selector
      2.Node Affinity and Node-Anti-Affinity
      3.Pod Affinity and Pod-Anti-Affinity
      4.Taints and Tolerations

>refer, https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/


>NODE SELECTOR_

>nodeSelector is a feild in pod spec, which defines the node on which we wants pod to get 
>schedule using lables attached to the node.

>it is the simplest form of recommandation for node selection constraint.

>it uses labels to select matching node on to which pod can be scheduled.  if the defined label does not exist
>pod will go in to pending state forever.

Assigning label to node_
> kubectl label node <nodename> <label>=<value>
> kubectl label node worker01 size=large

deleting the Node label
> kubectl label node <nodename> <label>-
> kubectl label node worker01 size-

apiVersion: v1
kind: Pod                                                    
metadata: 
  name: secret1
spec: 
  containers:
    - name: container5          
      image: nginx
      imagePullPolicy: Never

  nodeSelector:
      size: "large"    # all the pode with this selector will get schedule on the node with matching labels.

>make sure that node with specified label exist,  if label is not attached to node and we in pod specification
>gave the node selector then pod will remain in pending* state , as it will fails to find the node with label.  

#-------------------------------------------------------

>Node Name_

apiVersion: v1
kind: Pod
metadata:
  name: myapp
  labels:
    name: myapp
spec:
  containers:
  - name: myapp
    image: nginx
    ports:
      - containerPort: 80 

  nodeName: <node_name>    


>nodeName is a feild in pod spec, which defines the node name on which we wants pod to get schedule.
>it is simplest way to schedule the pod on perticular node using node selection constraiunts, but due
>to its limitations it is typuically not used.
>we can also assign the pod to master node using this method.

>if the named node does not exist in cluster, the pod will not run and in some cases may be automatically deleted.
>like in eks cluster, we do not have choice for selecting node names.

>the the named node does not have a resource to accommodate the pod , the pod will fail.

>node name in cloud enevironments like eks or gke are not always predictable or stable.

to get names of the nodes in our k8s cluster.
>kubectl get nodes -o wide




#--------------------------------------------------AFFINITY---------------------------------------------------

>There are two types of Affinity, 
>NODE AFFINITY and POD AFFINITY, which is again divided in Affinity and AntiAffinity. (read on link below)

>refer, https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity


>NODE AFFINITY..............1>
# Assigning label to node_
> kubectl label node <nodename> size=large

apiVersion: v1
kind: Pod                                                    
metadata:                               >NODE AFFINITY
  name: secret1
spec: 
  containers:
    - name: container5          
      image: nginx
      imagePullPolicy: Never           
 
  affinity:
    nodeAffinity:                                               # preffered but not required
      preferredDuringSchedulingIgnoredDuringExecution:          # is is called SOFT SCHEDULING
        - preference:
            matchExpressions :
              - key: size
                operator: In           > operator can be, In/NotIn/Exist/NotExist etc...
                values:                        >label size=large
                  - large             #array, multiple values can be given
          weight : 1

> weight :- Weight is used to define the prefference of node selection, when we give multiple Match Expressions and 
>           and there are multiple nodes that satisfies the affinity conditions. so K8S rates the nodes and based 
>           on its values assign weight to it.
>           its value varies between 1 to 100, based on weight it "preffered" the node to get schedule.
>           (only for preffered). 
>           K8S will schedule the pod on a node with Highest total weight score. 
>           i.e weight 80 is more desirable than weight 40.

>--------------------------------------------2>

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: secret1
spec:                                          #NODE AFFINITY
  containers:
    - name: container5          
      image: nginx 
      imagePullPolicy: Never
                                       # Required, not just preffered, it is HARD scheduling
  affinity:                                    
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:     
        nodeSelectorTerms:    
          - matchExpressions :                         # size=large
              - key: size
                operator: In
                values: 
                  - large


>In above both cases, even if we remove the node label after pod is already scheduled on it
>pod will keep running on it. As both cases, have "IgnoredDuringExecution" policy attahed to it.

>requiredDuringSchedulingRequiredDuringExecution  --> do not exist in current k8s version 
>preferredDuringSchedulingRequiredDuringExecution --> do not exist in current k8s version   

>preferredDuringSchedulingIgnoredDuringExecution  --> exist in k8s (soft scheduling)
>requiredDuringSchedulingIgnoredDuringExecution   --> exist in k8s (hard scheduling)



#--------------------------------------------POD-AFFINITY---------------------------------------------------

# Pod Affinty and Pod antiAffinity_

>In pod Affinty and Pod antiAffinity , Node will select , How the perticular Pod will be placed Relative to other 
>pods based on the rules defined under Node affinity and AntiAffinity in pod spec manifest file.
>i.e. Node will select the pod , which it wants to execute/run on it.

>In pod Affinty and Pod antiAffinity , allows you to specify rules about How pods should be placed relative to
>other pods running on perticular node.

>Pod Affinity, can tell the scheduler to locate a new pod on the same node as 'Other Pod', 
>if the label selector on the new pod matches the label on current pod.
>i.e. it will check the label of existing pod, so that new pod will get schedule on the 
>same node as existing pod is. so that both pods will be on same node.

>Pod Anti Affinity, can Prevent the scheduler from locating a new pod on the same node as pod with the same label,
>if the label selector on the new pod matches the label on the current pod.



>Pod Affinity_

apiVersion: v1
kind: Pod                                  # this is old/already existing pod
metadata:
  name: nginx-with-node-affinity
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx

  affinity: 
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: 
        nodeSelectorTerms: 
          - matchExpressions:
            - key: gpu 
              operator: Exists 
            #  value: amd  

>above node affinity, will schedule the pod on the node having label as 'gpu' , no matters whats its value
>may be. so out of multiple node having label 'gpu' , it will schedule the pod on any of those nodes.


apiVersion: v1
kind: Pod                           # this is new pod, and it will colocate with existing pod.
metadata:
  name: nginx-with-pod-affinity
spec:
  containers:
  - name: nginx
    image: nginx

  affinity: 
    podAffinity: 
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector: 
          matchExpressions:  
            key: app
            operator: Exists        # this will match the expression with labels of the existing pod.
            value: 
              - nginx
        topologyKey: gpu            # it will look for the node with label 'gpu', so that pods will colocate. 
   


>This pod affinity rule deploys the pod on Node , if it has GPU as the KEY and if it is already running a
>Pod that has labels mentioned as a labelSelector. 
>SO THAT NEW POD AND OLD POD WILL BE COLOCATE ON THE SAME NODE. IT WILL FIRST SEARCH FOR OLD POD WITH 
>ITS LABELS AND THEN IT WILL ALSO CHECK ITS TOPOLOGYKEY.


>topologyKey is the key for the node label. any label can be topologykey. 



#-------------------------------------------POD-ANTI-Affinity----------------------------------------------

> Using affinity , we can tell k8s engine, where to schedule the pod, but using AntiAffinity we can place 
> the condition to tell where to Schedule and Where to not Schedule the pods.

>Using AntiAffinity rule, we basically tell the k8s engine , where to not schedule the pod. so k8s engine will
>not schedule the pod as per labels/rules defined in antiaffinity rules.


>Pod AntiAffinity_

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    
>this pod will get schedule on any node, as we have not defined any rules for this pod to get schedule.

apiVersion: v1
kind: Pod                           # this is new pod, and it will not-colocate with existing pod.
metadata:
  name: nginx-with-pod-anti-affinity
spec:
  containers:
  - name: nginx
    image: nginx

  affinity: 
    podAntiAffinity: 
      prefferedDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm: 
          labelSelector:
            matchExpressions:  
            -  key: app
               operator: In        # this will match the expression with labels of the existing pod.
               value: 
                - nginx
          topologyKey: any-common-lable #kubernetes.io/hostname      
                                               
>                                               #we use common label, which every pod will have, becouse,  
>                                               #we do know the node on which our first pod will get deployed.


>Above Anti Affinity, rule states that , if the existing pod is already deployed on the node, then the pod with
>Anti Affinity rules must not get schedule on that node.

>This is achieved using Labels, in anti Affinity rules, we are defining the matchExpression rules,
>which searches for the the node, for the pod with matching label, if it found that pod is on that node,
>then the new pod will nt get scheduled on it.


>WE CAN USE BOTH AFFINITY AND ANTI-AFFINITY RULES TOGETHER, SO THAT DIFFERENT CONDITIONS CAN BE SATISFIED.
1.In general, we use AntiAffinity rules, when we do Not wants two pods to be co-located. So Anti Affinity
  will Repele Similar pods.
2.In general, we use Affinity rules, when we do wants two pods to be co-located. So Anti Affinity
  will Attracts Similar pods.

>-------end----




#-----------------------------------------k8s volumes-------------------------------------------------------- 

> Three types of volumes are :-> EmptyDir, HostPath, PersistantDisk.

>1)-------------emptyDir-----------------------

>if we delete the CONTAINER data will be preserved , and when new container is created by pod, all
>the data will be present in new container.

>i.e data volumes is created inside the POD, and will be available to Container inside that pod

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: vol1
spec: 
  containers:                     
    - name: container5          
      image: nginx
      imagePullPolicy: IfNotPresent

      volumeMounts:
        - mountPath: /data
          name: test-volume

  volumes:
    - name: test-volume  
      emptyDir: {}   

>data will remain inside the pod and will be available to the container inside that pod.
>this data will not be available to other containers on other pod or other pod on other machines.


>2)-------------HostPath-------------------------------

>if we delete the POD data will be preserved in directory in sync on Host Machine, 
>and when new POD is created it will have access to the data stored in dirctory on host.
>the data will be present in new POD inside the container.

>i.e data volumes is created inside the Host Machine, and will be available to POD on that machine.

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: vol2
spec: 
  containers:                
    - name: container5          
      image: nginx
      imagePullPolicy: IfNotPresent

      volumeMounts:
        - mountPath: /data
          name: test-volume

  volumes:
    - name: test-volume  
      hostPath:
        path: /tmp/data   # at this path on Host machine data will be synchronised with containers data
                              # first, >minikube ssh , there /tmp folder path will be available.

>if we delete the pod and if pod get scheduled on another machine, then that pod will not have access to data
>available on previous machine.


>3)-------------EKS ON AWS-------------------------------- > Persistance data.                             

>In above two cases , if pod is get schedule on any machine , other than its original machine in cluster data will 
>no longer be available to the pod.
>In this all we need a volume which is available to the all the machines in the cluster, Here AWS EKS comes to rescu.

>In this approch , we create a managed k8s cluster on aws eks, and create a ebs volume, which will remain
>available to the all nodes.  (managed cluster .i.e. AWS ITSELF WILL MANAGED MASTER NODE/CONTROL PLANE)

>If pod get schedule on a perticular node, then EBS volume will get attached to that node automatically and
>if we delete and create the pod , on anothe rmachines in cluster, then that EBS volume will automatically get
>attached to that machine. and this way pod will always have acces to the data.

>Create EKS cluster on K8s_
>1)install awscli
>2)with help of IAM user log in to aws account.
>3)start accessing aws services over awscli

#to create EKS cluster_
>eksctl create cluster --name <cluster-name> --node-type <type of ec2> --region <region> --node-zones <az>

>eksctl create cluster --name my-cluster --node-type t2.small --region ap-south-1 --node-zones ap-south-1a

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: vol2
spec: 
  containers:                
    - name: container5          
      image: nginx
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - mountPath: /data
          name: test-volume

  volumes:
    - name: test-volume
      awsElasticBlockStorage:                 # attach ebs volumes pod.
          volumeID: "vol-02c13c4470d26d461"
          fsType: ext4
      

>once our cluster get set up, it will also download the kube-config in to local machine. and we will 
>able to directaly use the kubectl commands on eks, No manual configuration is needed.

>----------------------------------------------------------------------------------------------------------


#--------------------POD LIVENESS AND READINESS PROBE-----------------------------

> refer, https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/

>--------------------------------Liveness Probe---------------------------------

>The kubelet uses liveness probes to know when to restart a container.
>The kubelet uses readiness probes to know when a container is ready to start accepting traffic.
>A Pod is considered ready when all of its containers are ready, i.e. when it passes the liveness probe.

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: live1
spec: 
  containers:                
    - name: cont1          
      image: nginx
      restartPolicy: Always        >Always(default), OnFailure, Never
      args: 
       - /bin/bash
       - -c
       - touch /tmp/test       # here , args will create a file test , this file will be checked by
      #  - sleep 10            # liveness probe at after 5 sec of container is ready and at regular interval of 3 sec.
      #  - rm -f /tmp/test     # as long this file is available for the probe to check, pod is consider as healthy.
      #  - sleep 20            # if we delete the file, then probe will no longer have acces to file and at next check, 
                               # probe will mark pod as unhealthy.
      livenessProbe:
        exec:                  
          command:
            - cat 
            - tmp/test
        initialDelaySeconds: 5
        timeoutSeconds: 3

>If the command succeeds, it returns 0, and the kubelet considers the container to be alive and healthy. 
>If the command returns a non-zero value, the kubelet kills the container and restarts it.    
>Once, liveness probes have failed, and the failed containers will be killed and recreated.    

> apart from "command" in liveness probe, we can also use "http" request and "TCP" socket for the probe.        
> read on, Startup probe..........

>If the liveness probe fails, the kubelet kills the container, and the container is subjected to its restart policy.
>If a container does not provide a liveness probe, the default state is Success. 


>--------------------------------Readiness Probe-------------------------------

>even through pod is created and live, it do not means that it is ready to serve the requests. App may take 
>time to be ready to serve the traffic. we do readiness probe to check the same.

>Note: Readiness probes runs on the container during its whole lifecycle.
>Caution: Liveness probes do not wait for readiness probes to succeed. 
>If you want to wait before executing a liveness probe you should use initialDelaySeconds or a startupProbe.

apiVersion: v1
kind: Pod                                                 
metadata: 
  name: live1
spec: 
  containers:                
    - name: cont1          
      image: nginx
      args: 
       - /bin/bash
       - -c
       - sleep 10
       - touch /tmp/test
                                  #liveness probe not necessary for readiness check.
      livenessProbe:
        exec:                  
          command:
            - cat 
            - tmp/test
        initialDelaySeconds: 10    # will check for liveness after 10 sec of pod creation, every 3 seconds.
        timeoutSeconds: 3

      readinessProbe:
        exec:
          command:
            - cat 
            - /tmp/test
        initialDelaySeconds: 20 # will check for readiness after 20 sec of pod creation every 5 seconds.
        timeoutSeconds: 5 

>If the readiness probe fails, the endpoints controller removes the Pod's IP 
>address from the endpoints of all Services that match the Pod.

>The default state of readiness before the initial delay is Failure . 
>If a container does not provide a readiness probe, the default state is Success 
      
>--------------------------------------------------------------------------------------------        


>read more_

>a)Containers_
1. Sidecar Container 
2. Ambassador Container 
3. Adapter Containers 
4. Init Containers 
5. Ephemeral Containers 

>b)Volumes_
1. Persistant Volume 
2. Persistant Volume Claim
3. EmptyDir
4. Host Path 


>c)Services_
  1. NodePort      (expose service out of cluser)
  2. LoadBalancer  (expose service out of cluser)
  3. ClusterIP     (expose service only within cluser)

  4. Ingress/Ingress_Resources/Ingress_Rule
  5. Ingress Controller 
  
  6. Port Forwarding
  7. External IP Service
  8. External Name Service
  9. Headless Service 

>d) K8S AutoScalling
>e) Kubernetes DashBoard
>f) Steteful Sets
>g) Service Account
>h) Role Based Access Control
>i) 